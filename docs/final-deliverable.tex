\documentclass[11pt]{article}

\usepackage{charter}
\usepackage{alltt}
\usepackage{url}
\usepackage{proof}
\usepackage{amsfonts}
\usepackage{underscore}
\usepackage{pstricks}
\usepackage{pst-node}
\usepackage{makeidx}

\include{cpp-macros}

\newcommand{\ie}{\emph{i.e.}}
\newcommand{\naive}{na\"\i{}ve}
\newcommand{\lbr}{\texttt{\{}}
\newcommand{\rbr}{\texttt{\}}}
\newcommand{\HOLfile}[1]{HOL:\texttt{#1}}

\title{A Formal Semantics for \cpp}
\author{Michael Norrish\\{\small \texttt{Michael.Norrish@nicta.com.au}}}
\date{}

\makeindex

\begin{document}
\maketitle
\tableofcontents

\section{Introduction}

This document attempts to present the substance of the mechanised
\cpp{} semantics that is developed in the accompanying HOL source
files (see Appendix~\ref{sec:sources}).  Those files sum to over
12$\,$000 lines; this document tries to cover both the important parts
in detail, and to describe the less important parts at a high level.

The HOL mechanisation itself is necessarily the only \emph{formal}
part of the deliverable.  This document quotes from the sources
liberally, but aims to make these relatively easily understood by
accompanying HOL extracts with English prose.  Where prose and rule
appear to conflict, this will almost certainly reflect a problem with
the prose and not the rule.  The rule has been type-checked, and in
some cases, will have also been validated to some extent.  (See
Section~\ref{sec:validation} for more on validation.)


\subsection{Phases of Understanding}

When we attempt to understand the meaning of a \cpp{} source file (or
``translation unit'' to use the standardese), this task is best broken
down into three phases.

When we begin, we have a sequence of parsed \emph{external
  declarations} (see the Standard's formal grammar in its Annex A:
Grammar Summary).  Throughout this semantics, we assume that this
sequence is well-formed syntactically, that some trusted compiler has
already checked the sources for syntactic errors of the sort compilers
can detect.  For example, this means that we assume that there are no
variables left undeclared, and that all the various expressions are
well-typed.  This is a not inconsiderable simplification of the basic
task, but it does seem fair to claim that such analysis is not as
interesting a problem.  This abstract syntax, as consumed and
manipulated by the semantic model, is described in
Section~\ref{sec:basic-types} below.

The first phase of understanding, or of ascribing meaning, is to do
what I refer to as ``name resolution''.  In this phase, bare names are
resolved into fully-qualified names wherever possible.  For example,
this phase turns a program such as
\begin{verbatim}
   int x;
   int f(int i) { return i + x; }
\end{verbatim}
into
\begin{verbatim}
   int ::x;
   int ::f(int i) { return i + ::x; }
\end{verbatim}
where the names declared in the top, global namespace (\texttt{x} and
\texttt{f}) have been replaced with unambiguous versions of their
names wherever they occur.  Note that the result is no longer valid
\cpp{} (it is illegal to use the explicit qualification in names being
defined), but the next phase of understanding does not expect valid
\cpp{} in any case.  In the presence of hierarchical namespaces, and
class namespaces inheriting from bases, this phase of work is not
entirely trivial.  It is described in some detail in
Section~\ref{sec:phase1}.

The second phase of the semantics is to deal with templates.  Template
resolution takes a translation unit with most of its name resolved,
and where the unit consists of ``ground'' (non-template) and template
declarations or definitions.  The ground definitions may refer to
various template classes or functions.  If so, the appropriate
template instantiations need to be made, producing fresh ground
declarations.  These new declarations need to first have their names
resolved (requiring a nested ``call'' to Phase~1), and may in turn
require more template instantiations.  In fact, template instantiation
may never terminate.  Templates are further described in
Section~\ref{sec:templates}.

The final phase of the semantics is execution, or ``dynamics''.  In
this phase, top-level declarations are executed, resulting in the
dynamic initialisation of variables, which can in turn result in the
execution of source-code.  The exact order of evaluation of external
declarations is allowed to vary (see \S3.6.2), and may or may not
precede the execution of a program's \texttt{main} function.  The
rules governing dynamic behaviour specify what is to occur when
execution does occur, but do not specify how various executions are
knitted together.  All of the model's dynamic rules are presented in
Section~\ref{sec:phase3}.

\subsection{How to Read HOL Extracts}

There are two main forms of definition used in HOL, definition of
functions specified equationally, and inductive definition of
relations.  The former look similar to definitions in functional
programming languages, with features such as pattern-matching and
recursion common.  For example, the following is the definition of a
factorial function
\begin{alltt}
   (FACT 0 = 1) \(\land\)
   (FACT (SUC n) = SUC n * FACT n)
\end{alltt}
The individual equations are separated by conjunction symbols because
the text of the definition becomes the statement of the theorem
characterising the behaviour of the new constant.  (If the recursion
is not well-founded, or if there is some other error in the quoted
text, then HOL does not allow the definition to proceed, and no
theorem is produced.)

Inductive relations allow the definition of systems of rules, common
in the definitions of operational semantics.  For example, one might
write a big-step rule for applicative order reduction in the
$\lambda$-calculus as
\[
\infer{\Gamma \vdash M\,N\;\Downarrow \;v}{\Gamma \vdash M \;\Downarrow\; (\lambda x. M_0) &
  \Gamma \vdash N \;\Downarrow\;v_0 & (x\mapsto v_0),\Gamma \vdash M_0 \;\Downarrow\; v}
\]
This should be read as stating that an application $M\,N$ reduces to a
value $v$ if $M$ reduces to an abstraction ($\lambda x. M_0$), if $N$
reduces to some value $v_0$, and if the body $M_0$ reduces to $v$,
while the bound variable $x$ is linked to $v_0$.

The same rule might be presented in HOL syntax as
\begin{alltt}
     apeval G M (LAM x M0) \(\land\)
     apeval G N v0 \(\land\)
     apeval ((x,v0) :: G) M0 v
   \(\Rightarrow\)
     apeval G (APP M N) v
\end{alltt}
The use of the conjunction and implication symbols makes the
propositional structure of the rule explicit.  Also, the relation
being defined is always the first symbol (\texttt{apeval} here) of the
conclusion.

Note also how the \texttt{apeval} constant of this example is applied
to its three arguments in ``curried'' $\lambda$-calculus or
functional-programming style.  This is in contrast with ``standard''
mathematical style, where functions are typically applied to arguments
in parentheses separated by commas, as in $f(x,y,z)$.  Such tuples do
also appear in the model, but sequences of function or predicate
arguments are often just separated by whitespace, as above.

Where quotations are made from underlying HOL sources, the origin of
the quotation will be identified using the form \HOLfile{thyname}.
The source code for theory \texttt{thyname} is available in the file 
\begin{alltt}
   holsrcs/thynameScript.sml
\end{alltt}
When compiled, the corresponding theory will have a readable
signature in the file
\begin{alltt}
   holsrcs/thynameTheory.sig
\end{alltt}

\section{Basic Types}
\label{sec:basic-types}

The most fundamental types in the semantics are those expressing the
basic abstract syntax of \cpp{}.  These are declared in a BNF-like
way.  For example, the declaration of \cpp{} types is (from
\HOLfile{types})
\begin{verbatim}
   CPP_Type =
     Void |
     BChar (* "Basic char" *) |
     Bool |
     Unsigned of basic_integral_type |
     Signed of basic_integral_type |
     Class of CPP_ID  |
     Float |
     Double |
     LDouble |
     Ptr of CPP_Type |
     MPtr of CPP_ID => CPP_Type | (* member pointer *)
     Ref of CPP_Type |
     Array of CPP_Type => num |
     Function of CPP_Type => CPP_Type list |
     Const of CPP_Type |
     TypeID of CPP_ID
\end{verbatim}
This definition allows recursion: for example, a \cpp{} type can be a
pointer to another \cpp{} type (using the \texttt{Ptr}
``constructor'').  In this simple prefix notation, the type of an
``array of ten pointers to \texttt{int}'', is written
\begin{verbatim}
   Array (Ptr (Signed Int)) 10
\end{verbatim}
(\texttt{Int} is one of the four possible values inhabiting
\texttt{basic_integral_type}, along with \texttt{Char}, \texttt{Short}
and \texttt{Long}.)

Similarly, a function taking two \texttt{int}s and returning a
\texttt{char} is written
\begin{verbatim}
   Function BChar [Signed Int; Signed Int]
\end{verbatim}

\paragraph{Identifiers}
In the presence of templates, identifiers can take on forms such as
\begin{verbatim}
   List<int>::fldname
\end{verbatim}

This means that identifiers are a type in the model that must in turn
be mutually recursive with the type of types.  In the example above,
the type \texttt{int} appears within an identifier.  It is also clear
that identifiers occur within types, because identifiers are the basis
for naming and referring to class types.

Therefore, we must add the following to the above definition of \cpp{}
types:
\begin{verbatim}
   CPP_ID = IDConstant of bool => IDComp list =>
                          IDComp

   ;

   IDComp = IDTempCall of string => TemplateArg list
          | IDName of string
\end{verbatim}
In other words, values of identifier type are constructed by applying
the function \texttt{IDConstant} to three arguments: a boolean
indicating whether or not this is an ``absolute'' identifier
(represented in the concrete syntax by prefixing it with \texttt{::});
a list of ID components and one final ID component.

An ID component might either be a simple name, or can be a simple name
applied to multiple ``template arguments''.  There are three sorts of
template arguments: types, templates and values; giving
\begin{verbatim}
   TemplateArg = TType of CPP_Type
               | TTemp of CPP_ID
               | TVal of TemplateValueArg
\end{verbatim}
So, the type above would be represented as
\begin{verbatim}
   IDConstant F [IDTempCall "List" [TType (Signed Int)]]
                (IDName "fldname")
\end{verbatim}


\paragraph{Expressions and Statements}

Expressions are specified in exactly the same way as types, with
constructors such as \texttt{Assign} (assignment), \texttt{Deref} (the
\texttt{*} or pointer dereferencing operator) and \texttt{New}.  The
abstract syntax need not be a perfect match for the concrete syntax.
For example, there is an \texttt{ExpTypeID} operator (for
\texttt{typeid} applied to an expression argument), and
\texttt{TyTypeID} for when \texttt{typeid} is applied to a type.

The rules presented in Section~\ref{sec:phase3} cover the dynamic
behaviours various expression forms.  The HOL declaration is in the
file \HOLfile{expressions}.

Statements are similar again (see \HOLfile{statements}), with
constructors such as \texttt{CIf}, \texttt{Ret} and \texttt{Block}.
However, the type here is rather more complicated because statements
not only include expressions but must be mutually recursive with other
syntactic categories: variable and class declarations, ``class
entries'' (those things that can appear with a \texttt{class}
declaration), and initializers (those things that appear in variable
declarations that also explicitly initialise the variable).

\subsection{Bytes \& Memory}
\label{sec:bytes-memory-states}

After specifying abstract syntax for programs, one must continue by
describing the state that is manipulated by the action of those
programs.   In fact, each of the three phases manipulates slightly
different states, and each will be detailed in the relevant sections.
However, there are a few general observations possible.

\paragraph{Bytes} The fundamental type in the dynamic semantics is
that of the byte.  (See \HOLfile{memory} for more on these matters.)
Using HOL4's support for $n$-bit words, it is possible to define a
type called \texttt{byte}, which is a word containing
\texttt{CHAR_BIT} many bits, where \texttt{CHAR_BIT} is a natural
number under-specified to be at least 8, but possibly more.

It is then possible to define representation and valuation functions
between the HOL types of integer and byte.  These functions are
partial, meaning that they can either return \texttt{NONE} to indicate
failure, or $\texttt{SOME}(v)$ to indicate the successful return of
the value $v$.  These functions capture, again in a suitably
underspecified way, how bytes can be translated into values, and how
those same values can be converted back into bytes.  For example, we
know that for the unsigned \texttt{char} type, all values in the range
$0$ up to $2^{\texttt{CHAR_BIT}}-1$ must have corresponding bit
patterns.  For types other than \texttt{char}, the use of functions in
both directions is perhaps not quite under-specified enough: it
assumes that if an implementation can represent a value at all, then
it will always represent a value in the same way.  For example, this
reduces, though does not eliminate, the opportunities for signed
zeroes to occur.

Each primitive type is given a fixed size (in numbers of bytes).  This
is done in an underspecified way so that again, one can only conclude
that there are enough bytes in an \texttt{int} value to represent the
mandated range of values (from $-(2^{16} - 1)$ up to $2^{16} - 1$).

In the model to come, the function most used is
\begin{verbatim}
   INT_VAL : CPP_Type -> byte list -> int option
\end{verbatim}
which attempts to interpret the given list of bytes as a value of the
provided type, and returns its integer value, if it has one.  The
function might return \texttt{NONE} if the list of bytes is of the
wrong length, or if it is not a valid bit pattern for the type.  (The
latter might occur if the required type is a pointer value and the
hardware checks such values for validity before even allowing them
into address registers.)

\paragraph{Memory}
Memory is represented as a function from natural numbers to bytes.
The address 0 is reserved as the representation for the null pointer.
Strictly speaking, one might imagine that the map should be from some
machine word (an array of four bytes, say) to bytes.  However, even
with the domain of the map being $\mathbb{N}$, any given program will
only be able to address a finite amount of memory because it will only
be able to generate a finite number of addresses.  (All pointer types
have a fixed, finite number of bytes making up their representations.)

\subsection{Hierarchical Environments}
\label{sec:hierarchical-environments}

To reflect the hierarchical nesting of namespaces and classes, the
model uses a type of hierarchical environments, giving maps from
structured names into information about those names.  These maps are
instances of a type called \texttt{fmaptree} (see \HOLfile{fmaptree}).
This type has one constructor:
\begin{verbatim}
   FTNode : 'value -> ('key |-> ('key,'value)fmaptree) ->
            ('key,'value)fmaptree
\end{verbatim}
The \texttt{|->} type operator returns finite maps, so
\texttt{FTNode} takes a value, and a finite map from keys to more
\texttt{fmaptree}s, and returns a new \texttt{fmaptree}.  This type
rather resembles the trie data structure.  Like lists and other
``container'' types, the \texttt{fmaptree} is polymorphic, both in the
type of keys, and the type of values.

The operation to lookup up the sub-tree at a particular key list position
is \texttt{apply_path}:
\begin{alltt}
  (apply_path [] ft = SOME ft) \(\land\)
  (apply_path (h::t) ft = if h \(\in\) FDOM (map ft) then
                             apply_path t (map ft ' h)
                          else NONE)

\end{alltt}

The functions \texttt{item} and \texttt{map} are also heavily used,
and return the value, and sub-trees of an \texttt{FTNode}
respectively.
\begin{verbatim}
   item (FTNode i fm) = i

   map (FTNode i fm) = fm
\end{verbatim}

In the particular context of \cpp{}, there are two sorts of
environment (see \HOLfile{environments}).  The first maps from
namespace components into values of type \texttt{envinfo}.  An
\texttt{envinfo} is a record of three components, each of which are
finite-maps.
\begin{verbatim}
   envinfo = <|
     varmap   : string |-> addr # CPP_ID # CPP_ID list ;
     typemap  : IDComp |-> CPP_Type ;
     classenv : IDComp |-> class_env
   |>
\end{verbatim}
The \texttt{varmap} is a map from variable names to their l-value
information.\footnote{It should be clear that an address is necessary
  to specify an object's identity.  The additional identifier and
  identifier list are used to store the dynamic information about
  class types that is necessary to implement polymorphism.  For more
  on this, see Section~\ref{sec:multiple-inheritance} below.}  The
domain of this map can be strings because only functions can have
template-structured names, and these do not live in memory in the same
way as objects.  The \texttt{typemap} maps ID components to types,
giving static information both for objects and functions.  The
\texttt{classenv} field gives information about any classes that might
be declared at this level of the namespace hierarchy.

The \texttt{environment} type is then an abbreviation for a
\begin{verbatim}
   (string, envinfo) fmaptree
\end{verbatim}
At each point in the namespace tree, indexed by a path (or list) of
names, there is an \texttt{envinfo} value about the objects and
classes stored in that scope.

The \texttt{class_env} type is an abbreviation for another sort of
\texttt{fmaptree}
\begin{verbatim}
   (IDComp, class_envinfo) fmaptree
\end{verbatim}
In other words, a \texttt{class_env} is a structured map, where the
components can be full-blown \texttt{IDComp} values.  This is
necessary because classes can be constructed from template calls
(whereas namespaces are necessarily identified by just strings).

The information attached to each node of a \texttt{class_env} is the
following record type:
\begin{verbatim}
   class_envinfo = <|
      (* ironically, the location of the static variables is 
         only available dynamically, as classes are 
         initialised *)
      statvars : string |-> addr # CPP_ID # CPP_ID list ;
      info     : state_class_info ;
      refs     : string # addr |-> addr # CPP_ID # CPP_ID list
   |>
\end{verbatim}
The \texttt{statvars} field records the same information for a class's
static variables as the \texttt{envinfo} records for normal
variables.  The \texttt{refs} field records per-class information
about reference members.  Both of these fields are only used
dynamically (as static variables are declared, and as new classes are
constructed respectively).  Finally, the \texttt{info} field records
the static information associated with a class. 



\section{Phase 1: Name Resolution}
\label{sec:phase1}

Name resolution must occur in a separate phase before dynamic
evaluation, and must rewrite declarations so that their name
dependencies are made explicit.  This is exemplified by the program in
Figure~\ref{fig:name-res-separate-phase}, where the name \texttt{x}
that occurs in the function \texttt{ns1::f} must be a reference to the
\texttt{x} that occurs in the outermost, global namespace.  A \naive{}
execution of the sequence of declarations in the program would put the
global \texttt{x} into its namespace, and then enter namespace
\texttt{ns1}, where it would first declare the function \texttt{f},
and then the second \texttt{x}.  A later call to \texttt{ns1::f} would
correctly open up the entirety of the namespace, and immediately mask
the global \texttt{x} with \texttt{ns1::x}, causing the evaluation of
the body to proceed erroneously.
\begin{figure}[htbp]
\begin{verbatim}
   int x = 3; 
   namespace ns1 { 
     int f(int n) { return n + x; }
     int x = 2;
   }
\end{verbatim}
\caption{A program demonstrating the need to have name resolution be a
  separate phase before dynamic evaluation.}
\label{fig:name-res-separate-phase}
\end{figure}

Even if one imagined a version of the dynamics that did perform name
resolution as it evaluated declarations, this semantics would still
need to transform the body of \texttt{ns1::f} to include the correct
reference to \texttt{::x}.  The rest of this section of the report
will describe the relation that turns the program in
Figure~\ref{fig:name-res-separate-phase} into 
\begin{verbatim}
   int ::x = 3;
   int ::ns1::f(int n) { return n + ::x; }
   int ::ns1::x = 2;
\end{verbatim}

\subsection{The Name Resolution State}
\label{sec:name-resol-state}

In order to track the current set of names that are in scope, Phase~1
uses two environments, as per
Section~\ref{sec:hierarchical-environments} above, to capture what is
known about the current nested scopes, as well as some extra fields to
describe the names that are \emph{visible}.  For example, in the
program of Figure~\ref{fig:name-res-separate-phase}, the global
\texttt{x} is visible when \texttt{::ns1::f} is defined, but there is
no \texttt{x} in the namespace \texttt{::ns1}, at least at that stage.

The HOL definition (see \HOLfile{name_resolution}): 
\begin{verbatim}
   P1state = <|
     current_nspath : string list ;
     dynclasses : string |-> bool # IDComp list # 
                             TemplateArg list ;
     dynobjs : string |-> bool # IDComp list # 
                          TemplateArg list # dynobj_type ;
     dynns : string |-> string list ;
     global : state ;
     accdecls : ext_decl list 
   |>
\end{verbatim}
The three \texttt{dyn} fields record what names are visible in three
different categories: namespaces~(\texttt{dynns}),
classes~(\texttt{dynclasses}) and objects~(\texttt{dynobjs}).  Each
maps to information sufficient to provide an exact location for the
name.  

In the case of namespace names, it is enough to provide a path from
the root, and such a path will just be of strings.  For objects and
classes, the path has to be of ID components because objects and
classes can be nested inside classes (and classes might be template
classes).  The boolean also records whether or not the name is local
or non-local.  The list of \texttt{TemplateArg} values records any
template parameters that the name may be associated with if it is a
template.  Finally, in the case of objects, it is also necessary to
record what sort of object the name is.  The options are given in the
type \texttt{dynobj_type}: 
\begin{verbatim}
   dynobj_type = 
      dStatMember | dMember | dVirtualMember | dNormalObj
\end{verbatim}
Note that in this context an ``object'' might actually be a function
(and it is functions that provide the interest); elsewhere funtions
are not considered objects because they don't occupy allocated
memory. 

The \texttt{global} field of a \texttt{P1state} is a state from the
dynamic semantics.  In Phase~1 the vast majority of the information
stored in such a state is ignored; the state is part of the
\texttt{P1state} only for its two environments, which are accessed as
the fields \texttt{genv} (the global environment), and \texttt{env}
(the local environment). 

Finally, the \texttt{accdecls} field records the accumulating
translated declarations.  It is this that provides the final output of
Phase~1.  

\subsection{Template Names}

One significant issue is the resolution of names in templates.  When a
template definition is instantiated, it is important to specify how
the names occuring in the template bind.  Typically, such names might
bind to global names that are in scope at the point of the template's
definition, or to member functions associated with the template
argument.

The rule is actually fairly straightforward, at least in principle:
function names are allowed to bind to template names when the
statically determined types of the function arguments refer to
template parameters.  Such names have to be left alone in Phase~1.



\section{Phase 2: Templates}

\label{sec:templates}

In this section, I describe how the semantics models templates.  I
have been inspired by Siek and Taha~\cite{DBLP:conf/ecoop/SiekT06},
though as I shall discuss, the dynamics of their model is too
simplistic for the full language of templates, in particular handling
template parameters that are themselves references to templates
(``higher order templates'' if you will).\footnote{Siek and Taha do
  model \texttt{typedef} declarations within classes, which I do not.}

One might imagine that it be possible to treat templates at
``run-time'', as if one had written a one-pass \cpp{} interpreter.
However, the example in Figure~\ref{fig:templates-not-interpretable}
demonstrates that such a goal is impossible, that one would have to
write a two pass interpreter at the very least.  Any reference to the
figure's \texttt{List} template, perhaps within a function that was
called at a great stack depth, causes the need to statically
initialise the global \texttt{node\_count} before the program even
begins.  A \naive{} interpreter that attempts to execute the program
source as it sees it, will come unstuck.

As the interpreter sees the template declarations above, it does
nothing.  Then it performs its global declarations, and jumps into
main.  At this point it has already failed to do the right thing.
Instead, the putative interpreter would have to scan the whole program
for template applications so that it can generate the appropriate
global variable initialisations.  This is no better than explicitly
pre-compiling templates, so I have adopted an explicit two-phase
compilation approach.

\begin{figure}
\begin{verbatim}
  template <class T> class List {
    T item;
    List *next;
    static int node_count;
  };

  template<class T> int List<T>::node_count = 0;
\end{verbatim}
\caption{A program demonstrating the difficulty of interpreting
  templates.}
\label{fig:templates-not-interpretable}
\end{figure}



Finally, we need to specify the possible sorts of arguments that can
be passed to templates.  The Standard is quite explicit
here~\cite[\S14.3~para~1]{cpp-standard-iso14882}, there are three
sorts of arguments: types, templates, and ``non-type, non-template''
arguments (meaning references to objects with linkage, or numbers).
Thus:
\begin{verbatim}
   TemplateArg = TType of CPP_Type
               | TTemp of CPP_ID
               | TVal of TemplateValueArg
\end{verbatim}
Finally, there are four different sorts of non-type, non-template
arguments~\cite[\S14.3.2~para~1]{cpp-standard-iso14882}:
\begin{verbatim}[t]{cl}
   TemplateValueArg =
       TNum of int
     | TObj of CPP_ID
         (* id is of suitable global (one that has
            linkage etc). *)
     | TMPtr of CPP_ID => CPP_Type
     | TVAVar of string (* => CPP_Type *)
         (* can have a value (of the given type)
            substituted for this *)
\end{verbatim}
This presentation is slightly simplified because the standard also
allows arithmetic on these arguments, where this is appropriate
(between \texttt{TNum} and \texttt{TVAVar} parameters).

This is a complicated type, and it does assume that some previous
phase of analysis has determined which references are to class types,
and which are to namespaces.  In this way, something like
\texttt{x::y::z} can be split up to include namespaces within a
\texttt{TopName} component, and sub-fields or sub-classes within
\texttt{IDFld} structuring.

Similarly, a string such as \texttt{x::t<int>} is ambiguous without
some static analysis being performed to determine whether \texttt{x}
is a namespace or a class.  If \texttt{x} is a class, then \texttt{t}
must be a template member function, but if it is a namespace, then
\texttt{t} might be either a class template or a function template.

Further note that there can be at most two template calls within an
identifier.  If there are two, then the outermost is a class template,
and the second will occur last, and will be a template member
function.

\subsection{Instantiation and Matching}

(This seection describes formalisation done in
\HOLfile{instantiation}.) 

\medskip
\noindent Types and identifiers can be \emph{instantiated}: mappings
from variable names to values are applied over the structure of the
value (type or identifier), and occurrences of variable names are
replaced by the appropriate element from the range of the function.
Because there are three sorts of variables (corresponding to the three
different sorts of template argument), an instantiation is actually a
triple of functions (one for each sort of variable).

In Siek and Taha~\cite{DBLP:conf/ecoop/SiekT06}, instantiation is a
very elegant operation.  In a more faithful model of more of \cpp,
more complexities intrude.  In addition to the need for three
mappings, the model must also accept that instantiation can result in
an invalid result.  Instantiation must become partial, which is
modelled by making the types of the various instantiation functions be
of the form
\[
\texttt{inst<}\tau\texttt{>} : \mathit{substitution} \to \tau
\to \tau\;\textsf{option}
\]
The partiality arises at the lowest level, as in the following
example:
\begin{verbatim}
   template<class T> void f<T>(int x) { T::staticfield = x; }
   void g() { f<int>(3); }
\end{verbatim}
This must be an error because it is not sensible to write
\texttt{int::staticfield}.  (Other type substitutions may also cause
this to be an error, but this error can be detected as the
substitution is done, without any need to lookup information about the
argument.)

The partiality of instantiation does not prevent us from defining a
partial order over types, such that $\tau_1 \leq \tau_2$ when $\tau_2$
is a more specialised/instantiated version of $\tau_1$.  As in Siek
and Taha~\cite{DBLP:conf/ecoop/SiekT06}, we can prove reflexivity,
transitivity, and antisymmetry (up to renaming of free variables).

Given the partial order, it is straightforward to find the best match
amongst a set of template definitions for a given template call.

\subsection{Program Instantiation}

Siek and Taha have an elegant model for program instantiation.  A
program is a sequence of definitions (of classes, and of static member
functions).  A definition may cause an existing template to be
instantiated because of a reference to that template within the
definition.  When a member function definition is encountered, if its
body includes a reference to other functions, these functions may need
to be instantiated.

For example, when analysing the program in Figure~\ref{fig:taha-prog},
the Siek and Taha's model will see the reference to
\texttt{Foo<T*>::f()} in the definition of \texttt{Bar<T>::g()} and
instantiate the definition of \texttt{Foo} (it knows that it does not
already have an instantiation for a type of the form \texttt{Foo<T*>}).
This instantiation will result in a template definition (one with free
variables), which may or may not be required in the rest of the
program.
\begin{figure}
\begin{verbatim}
   template <class T> class Foo { static int f(); };
   template <class T> int Foo<T>::f() { return 3; }

   template <class T> class Bar { static int g(); };
   template <class T> int Bar<T>::g() { return Foo<T*>::f(); }
\end{verbatim}
  \caption{In Siek and Taha's model, the definition of
    class \texttt{Foo<T>} will get instantiated to provide a definition
    of class \texttt{Foo<T*>} when a reference to that type is seen
    inside the definition of \texttt{Bar<T>::g}.}
\label{fig:taha-prog}
\end{figure}

This model breaks down in the presence of template parameters that are
templates because it becomes impossible to determine the dependencies
of a template definition.  In the program in
Figure~\ref{fig:taha-problem}, it is impossible to tell what
definition should be instantiated when processing the definition of
\texttt{Baz<A>::g}.  In the presence of template parameters, Siek and
Taha's model is too eager.

\begin{figure}
\begin{verbatim}
   template <class T> struct Foo { static int f(); };
   template <class T> int Foo<T>::f() { return 3; }

   template <class T> struct Bar { static int f(); };
   template <class T> int Bar<T>::f() { return 4; }

   template <template <class> class A> struct Baz {
     static int g();
   };
   template <template <class> class A>
   int Baz<A>::g() {
     return A<int>::f();
   }

   int main() { return Baz<Foo>::g(); }
\end{verbatim}
  \caption{Siek and Taha's model's early instantiation breaks down
    when it sees the definition of \texttt{Baz<A>::g}.  At this point,
    it can not tell which template is being instantiated in the body.
    By way of contrast, my model doesn't instantiate anything until it
    sees the definition of \texttt{main}.
    (This program is available as
    \texttt{notes/siek-taha-tempvar.cpp})}
\label{fig:taha-problem}
\end{figure}

My model only performs instantiations when there is a ground instance
to drive the instantiation.  Otherwise, it is similar to what is
presented in Siek and Taha.  In particular, it is important to avoid
instantiating member functions unless they are called for.

The model in the \texttt{templates} script is based around a working
state of four components: Templates, Residuals, Needs and
Declarations.  The Templates are those declarations encountered so far
which are of templates.  The Residuals are those ``resolved''
declarations that have either been encountered, or which have been
instantiated from Templates, and are thus ``ground'' or variable-free.
A resolved declaration is one that has had all of its dependencies
appropriately instantiated, or at least recorded.  The Needs are those
declarations that have been required for a resolution, and for which
there have been template declarations, but no definitions.  Needs can
be functions (class-members, including constructors and destructors,
or normal top-level functions), and static data members of classes.
Finally, the Declarations are the declarations that still need to be
processed, coupled with a number specifying how much work has been
done to the declaration so far (all declarations start at level 0).

The Declarations component of the instantation state can grow as well
as shrink, and it is this that can cause compilation to fail to
terminate.

Values in the Declarations component can be declarations or
definitions, and can be of classes, variables and functions.  The
behaviour on encountering declarations of ground entities is rather
involved, and detailed in Table~\ref{tab:ground-decls}.  The basic
idea is that any ground definition can cause the instantiation of
templates.  Such instantiations are always governed by the requirement
that the best match is found.  Successful instantiations are stored in
the Residuals component, so duplicate work is avoided.

Instantiations are done in two phases.  First any template classes
required are instantiated.  This instantiation can in turn prompt
further instantiation because a template class can have template
members, or template ancestors.  if the required types can be
instantiated without causing an infinite loop, the original definition
will eventually return to the top of the Declarations list, but at
Level 1.  This time, the definition is scanned to see if it depends on
any functions or static variables.   These references may or may not
have definitions (though they will all at least have declarations).
Those with definitions are instantiated and put onto the list of
Declarations.  Those without end up in Needs.

When a template declaration is encountered, this is put straight into
the Templates component.  When a template definition is encountered,
this may resolve an outstanding Need, so it is checked against all
current Needs.  Those that can instantiate against the new Template
definition are put onto the Declarations list.

\begin{table}
\hspace{-3em}
\begin{tabular}{lp{0.32\textwidth}p{0.32\textwidth}p{0.32\textwidth}}
  & \multicolumn{1}{c}{\textbf{Function}} &
  \multicolumn{1}{c}{\textbf{Class}} &
  \multicolumn{1}{c}{\textbf{Variable}} \\
  \textbf{Decl.}
  &
  \small \textbf{L0:}
  Add declarations of any referred to template types
  to the list of Declarations still to be processed.
  &
  % Class
  \small \textbf{L0:}
  If not already in Residuals, and a template application, find best
  instantiation for this type and add its instantiated definition
  (less any member function definitions) to
  list of Declarations still to be processed.
  &
  \small \textbf{L0:} Declarations can only be of non-template variables
  (template variables are declared inside class definitions).  Add to
  Residuals and continue.
  \\ \\
  \textbf{Defn.}
  &
  % Function 0
  \small\textbf{L0:} If not already present in Residuals, extract any
  template
  types, and put declarations of
  these into Declarations, ahead of this function definition at
  Level~1.
  &
  % Class 0
  \small \textbf{L0:} Extract any template types from the data-member
  definitions, and put Level~0 declarations for these into
  Declarations, followed by this declaration at Level~1.
  &
  % Variable 0
  \small \textbf{L0:} Add any referenced template types to
  declarations at Level~0 (such types may occur in the initialising
  expression for the variable), followed by the same declaration at
  Level~1.
  \\[1ex]
  &
  % Function 1
  \small \textbf{L1:} Extract any function and static
  variable references, and
  add those with definitions (once instantiated with best match)  to
  Declarations at Level 0.  Those
  functions or static variables without definitions are added to
  Needs.
  &
  % Class 1
  \small \textbf{L1:}
  Required types have been dealt with: now check for definitions of
  any static data members.  Those not present are added to Needs.
  Best matched members are added to Declarations at Level~0.
  &
  % Variable 1
  \small \textbf{L1:} Check initialising expression for references to
  template functions, or other static variables.  Add instantiated
  definitions (for those with definitions) to
  Declarations list at Level~0.  Add others to Needs.
\end{tabular}
\caption{Behaviour of program instantiation on ground declarations}
\label{tab:ground-decls}
\end{table}

\paragraph{Caveat} Unlike Siek and Taha's model, there hasn't been any
validation of my model of program instantiation, whether by the proof
of theorems, or by simulation of concrete examples.





\section{Phase 3: Dynamics}
\label{sec:phase3}

Much of the core semantics in this section is based on the C semantics
presented in my PhD thesis~\cite{Norrish98}.  In particular, details
of the way in which side effects are created and applied remain the
same, as does the use of an evaluation context, and the way in
unspecified order of evaluation is handled. 

\subsection{Dynamic States}
\label{sec:dynamic-states}

The dynamic semantics updates values of the type \texttt{state} (see
\HOLfile{states}), given in full in Figure~\ref{fig:state-type}.  The
first four components of the state are sets of addresses.  The
\texttt{allocmap} and \texttt{initmap} sets are from my C
model~\cite{Norrish98}, and record which addresses have been allocated
and initialised, respectively.  The new field \texttt{hallocmap} is
necessary to allow memory to be allocated on the heap, and for its
life-span to persist beyond the end of the current block.  The second
new field, \texttt{constmap} records which memory has been allocated
with the \texttt{const} \emph{cv}-qualifier.  (Updating such memory
causes undefined behaviour.)

\begin{figure}[htbp]
\footnotesize
\begin{verbatim}
   state = <| 
     allocmap : addr -> bool ;  (* the set of stack-allocated addresses *)
     hallocmap: addr -> bool ;  (* the set of heap-allocated addresses *)
     constmap : addr -> bool ;  (* the set of read-only addresses *)
     initmap  : addr -> bool ;  (* the set of initialised addresses *)

     fnmap    : CPP_ID |-> fn_info ;
                (* map from function 'names' to type information about
                   the given functions *)
     fnencode : CPP_ID |-> byte list ;
                (* map encoding function 'name' as a byte sequence
                   so that its address can be stored in memory *)
     fndecode : byte list |-> CPP_ID ;
                (* map inverting fnencode *)

     genv: environment ; (* non-local environment *)
     env : environment ; (* local version of the above *)

     locmap   : addr -> byte ;
                (* memory.  Domain might also be ( void * ) words *)

     stack    : (environment # CExpr option) list ;
                (* stack of environment and this info.  Updated
                   as blocks are entered and left *)

     thisvalue: CExpr option ;
                (* the value (i.e., this will always be an ECompVal
                   with a pointer value) of the this expression *)

     blockclasses : constructed list list ;
     exprclasses  : construction_locn list list
       (* the stack of objects that need to have destructors
          called.  First field is for automatic objects that have
          block-delimited lifetimes.  Second is for temporary
          objects that need to be destroyed at the end of the
          full enclosing expression *)
     ;

     current_exns : CExpr list  
                    (* stack of exceptions that might be subjected 
                       to a bare throw *)
   |>
\end{verbatim}
  \caption{The HOL type of dynamic state.  There are two environment
    values, \texttt{genv}, and \texttt{env}.  The former is for
    non-local, persistent identifiers, the latter for local
    identifiers.  Because there is no such thing as a local namespace,
    there will only be a top-level node in the \texttt{env} field
    (which may, however point to an arbitrarily deep
    \texttt{class_env}).}
\label{fig:state-type}
\end{figure}

\subsection{The Dynamic Relation}

\newcommand{\mng}{\texttt{mng}}

The fundamental relation in the dynamics semantics is \mng{} (or
``meaning''), which is a binary relation on states and abstract syntax
forms.  For reasons to do with the prevention of function call
interleaving (explained below in Section~\ref{sec:small-step-stmts}),
this one relation is used for both expression and statement forms.
(One might otherwise imagine two mutually recursive relations: one for
statements and the other for expressions.)

Thus the type of \mng{} is 
\begin{alltt}
   : (state # ExtE) -> (state # ExtE) -> bool
\end{alltt}
making it a binary relation on pairs of states and \emph{extended
  expressions}.  An extended expression is either
\begin{itemize}
\item a syntactic expression coupled with a side effect information
  record (containing the three fields, \texttt{update\_map},
  \texttt{ref\_map} and \texttt{pending\_ses}, ($R$, $\Upsilon$ and
  $\Pi$ in the terminology of my thesis)); or
\item a statement coupled with a continuation, which latter is a
  function that takes a value and returns an expression.  This latter
  is used to recreate the expression in which the function call
  that generated the statement occurred.  Also, all expressions within
  statements (such as those that appear as guards in loops and
  \texttt{if}-statements), are actually extended expressions.
\end{itemize}

In \HOLfile{statements}, the declaration of extended expression
(\texttt{ExtE}) is thus mutually recursive with the type of
statements:
\index{ExtE@\texttt{ExtE}}
\index{EX@\texttt{EX}}
\index{ST@\texttt{ST}}
\begin{verbatim}
   ExtE = EX of CExpr => se_info
        | ST of CStmt => conttype
\end{verbatim}

Most of the time reduction occurs between expressions and expressions,
or between statements and statements, allowing one to imagine that one
has the $\rightarrow_e$ and $\rightarrow_s$ from the C semantics. 
For example, when evaluating expressions, rules in the dynamics have
conclusions of the form
\begin{alltt}
   mng (s0, EX e0 se0) (s, EX e se)
\end{alltt}
where \texttt{s0} and \texttt{s} are the initial and final states;
\texttt{e0} and \texttt{e} are the initial and final expression forms;
and \texttt{se0} and \texttt{se} are the initial and final ``side
effect records''.

Similarly, when evaluating statements, conclusions are typically of
the form 
\begin{alltt}
   mng (s0, ST st0 c) (s, ST st c)
\end{alltt}
where \texttt{st0} and \texttt{st} are the initial and final statement
forms.  Note that the continuation (\texttt{c} above) never changes
within statement evaluation, meaning that statement rules will always
actually repeat the given continuation from initial to final tuple.

\subsection{Special Syntactic Forms}
\label{sec:spec-synt-forms}

In the abstract syntax types representing both expressions and
statements, I have added special forms that only arise as a result of
evaluation and could never be seen in an input program.  The most
important of these are the forms for representing values and l-values
within the expression type.  

\index{ECompVal@\texttt{ECompVal}}
The \texttt{ECompVal} constructor has type 
\begin{alltt}
   : byte list -> CPP_Type -> CExpr
\end{alltt}
and represents values as sequences of bytes, coupled with their type. 

\index{LVal@\texttt{LVal}}
The \texttt{LVal} constructor is used to represent l-values, and has
type 
\begin{alltt}
   : addr -> CPP_Type -> CPP_ID list
\end{alltt}
Thus, an l-value is represented by a combination of its base address
and its type, along with the list of identifiers that allow us to
represent the dynamic types of basic object orientation; see
Section~\ref{sec:basic-oo} below.

In addition, there is an analogue to \texttt{LVal} for functions,
called \texttt{FVal}.  This represents the identity of a function, and
has type: 
\begin{alltt}
   : CPP_ID -> CPP_Type -> CExpr option -> CExpr
\end{alltt}
A function is identified by its name, its type, and if a (non-static)
member function, the expression denoting the class object for which it
is to be called.

\index{UndefinedExpr@\texttt{UndefinedExpr}}
Finally, there is the special value \texttt{UndefinedExpr} used to
represent the occurrence of undefined behaviour within an expression.

\subsection{Simple Expression Rules}
\label{sec:simple-expr-rules}

\paragraph{Literals} We begin with two rules for literals.  We don't
have any rules for other literal forms, such as floating point
constants, though it is clear what they would look like.
\index{Cnum@\texttt{Cnum}}
\begin{alltt}
(* RULE-ID: number-literal *)
     (REP_INT (Signed Int) n = SOME bl)
   ==>
     mng (s, EX (Cnum n) se) 
         (s, EX (ECompVal bl (Signed Int)) se)
\end{alltt}

The only difference with character constants is that the underlying
number is pushed into a different sized space:
\index{Cchar@\texttt{Cchar}}
\begin{alltt}
(* RULE-ID: char-literal *)
     (REP_INT BChar n = SOME bl)
   \(\Rightarrow\)
     mng (s, EX (Cchar n) se) (s, EX (ECompVal bl BChar) se)
\end{alltt}

\index{Var@\texttt{Var}}
\paragraph{Variables} Looking up object variables becomes a little
complicated in the presence of references and object orientation. 
\begin{alltt}
(* RULE-ID: var-to-lvalue *)
     (lookup_type s vname = SOME ty0) \(\land\)
     object_type ty0 \(\land\)
     (lookup_addr s vname = SOME (a,cnm,p)) \(\land\)
     (ty = if class_type ty0 then Class cnm else ty0)
   \(\Rightarrow\)
     mng (s, EX (Var vname) se) (s, EX (LVal a ty p) se)
\end{alltt}
\index{lookup_type@\texttt{lookup_type}}
The call to \texttt{lookup_type} determines the variable's static
type, which will have been set in the appropriate part of the state
when the variable was declared.  The second premise checks to see that
the variable is of object type.  If so, the variable will have an
address.  Accompanying the address is information (\texttt{cnm} and
\texttt{p}) that gives dynamic type information if the object is of
class type.  

It may not be clear how a variable may come to have a dynamic type
that is separate from its static type.  In fact, this is only possible
in the presence of references, which are treated as aliases for real
variables.  Thus, in a function such as 
\begin{verbatim}
   int f(C &c) { return c.memfn(); }
\end{verbatim}
the variable \texttt{c} is initialised to ``point at'' some existing
variable, and the address maps are set up so that \texttt{c} is indeed
a perfect alias for some existing l-value.  But, the argument may in
fact have been a derived class of \texttt{C}, and so \texttt{c}'s
dynamic type won't be the same as its static type.

Variables can also denote functions:
\index{FVal@\texttt{FVal}}
\begin{alltt}
(* RULE-ID: var-to-fvalue *)
     (lookup_type s vname = SOME ty) \(\land\)
     function_type ty \(\land\)
     vname IN FDOM s.fnencode
   \(\Rightarrow\)
     mng (s, EX (Var vname) se) (s, EX (FVal vname ty NONE) se)
\end{alltt}


\paragraph{Contextual Evaluation}
\index{valid_econtext@\texttt{valid_econtext}}
Just as in the C semantics, most nested evaluation of expressions is
mediated through one rule:
\begin{alltt}
(* RULE-ID: econtext-expr *)
     mng (s0, EX e0 se0) (s, EX e se) \(\land\)
     valid_econtext f
   \(\Rightarrow\)
     mng (s0, EX (f e0) se0) (s, EX (f e) se)
\end{alltt}
Here, \texttt{f} is a function of type \texttt{:CExpr->CExpr},
but restricted by the predicate \texttt{valid_econtext}.  This
predicate restricts where evaluation can occur.  For example, a
function satisfying \texttt{valid_econtext} would be 
\[
\lambda e.\;\;\texttt{CAnd}\;e\;e_2
\]
for all possible values $e_2$.  Such a function allows reduction to
occur to the left of the \texttt{CAnd} constructor (\ie,
\texttt{\&\&}).  The corresponding function with its ``hole'' on the
right of the \texttt{CAnd} is not a valid context function.

\index{UndefinedExpr@\texttt{UndefinedExpr}}
If an undefined behaviour occurs, this is reflected by having the
expression that caused it become the special \texttt{UndefinedExpr}
value.  This value can rise to the top of any expression:
\begin{alltt}
(* RULE-ID: econtext-propagates-undefinedness *)
     valid_econtext f
   \(\Rightarrow\)
     mng (s, EX (f UndefinedExpr) se) (s, EX UndefinedExpr se)
\end{alltt}

The notion of where a function l-value decays into a pointer to a
function is also controlled by a context:
\index{valid_fvcontext@\texttt{valid_fvcontext}}
\begin{alltt}
(* RULE-ID: fcontext *)
     fnid IN FDOM s.fnencode \(\land\)
     (s.fnencode ' fnid = bytes) \(\land\)
     valid_fvcontext f
   \(\Rightarrow\)
     mng (s, EX (f (FVal fnid ty NONE)) se) 
         (s, EX (f (ECompVal bytes (Ptr ty))) se)
\end{alltt}
The definition of \texttt{valid_fvcontext} is 
\begin{alltt}
   valid_fvcontext f =
      valid_econtext f \(\land\)
      \(\forall\)args. \(\neg\)(f = \(\lambda\)f'. FnApp f' args)
\end{alltt}
stating that a function l-value can decay as in the rule above, as
long as it is not at the head of a function application.

\index{valid_lvcontext@\texttt{valid_lvcontext}}
\index{lval2rval@\texttt{lval2rval}}
Finally, there is the rule allowing normal l-values to decay into
their r-value forms (the ``l-value to r-value conversion''):
\begin{alltt}
(* RULE-ID: lvcontext *)
     valid_lvcontext f \(\land\)
     lval2rval (s0,e0,se0) (s,e,se)
   \(\Rightarrow\)
     mng (s0, EX (f e0) se0) (s, EX (f e) se)
\end{alltt}
The \texttt{lval2rval} relation can result in an
\texttt{UndefinedExpr} if the l-value causes a reference to a value
that has already been updated within the same phase of execution, as
might happen in the expression \texttt{i++ + i} for example.
Otherwise, if the l-value denotes an object not of class type, the
l-value turns into a list of bytes (an \texttt{ECompVal}), ready for
further manipulations to occur.

\paragraph{Operators} The rules governing the behaviour of the
standard operators are as in the original C semantics.  

\subsection{Statements in a Small-step Style}
\label{sec:small-step-stmts}

One might imagine that stating the statement part of a dynamic
semantics in a small-step style should be easy.  The literature
contains many examples of how to express constructs such as
\texttt{while} and \texttt{if} in this style.  However, this it is not
as straightforward as one might think because of the need to prevent
function bodies from interleaving.

Imagine a program such as that in Figure~\ref{fig:two-functions}, and
how one might evaluate the return-expression in \texttt{main}.  If one
simply expanded the bodies of the called functions into the expression
as the functions were ready to be called, one would be permitting the
simultaneous evaluation of the bodies of \texttt{f} and \texttt{g}.
But the \cpp{} standard explicitly forbids this (\S1.8~fn8), and the C
standard also hints that it is forbidden.
\begin{figure}[htbp]
\begin{verbatim}
   int global;
   int f(int x) { return global * 2 + x; }
   int g(int y) { while (y > 0) { global += 2; y--; } }

   int main(void) {
     global = 10;
     return f(6) + g(10);
   }
\end{verbatim}
\caption{Where Functions Must Not Interleave}
\label{fig:two-functions}
\end{figure}

One has to arrange the semantics so that expression evaluation can
continue non-deterministically until a function call is encountered
and the function call is made (after arguments have been evaluated).
At this point, all further evolution of the program must occur within
the function body, no matter how deeply nested the function call may
have been within an enclosing expression.  (This problem does not
occur if statement evaluation is big-step because the hypothesis in
the expression rule for a function call would be a statement rule that
required the complete evaluation of the function body.)

\index{FnApp@\texttt{FnApp}}
However, there are four important rules that mix the types of extended
expression.  The following rule governs (normal) function calls: 
\begin{alltt}
(* RULE-ID: global-function-call *)
     find_best_fnmatch s0 fnid (MAP valuetype args) 
                       rtype params body \(\land\)
     (pdecls = MAP (\(\lambda\)((n,ty),a). 
                       VDecInit ty (Base n)
                                   (CopyInit (EX a base_se)))
                   (ZIP (params, args)))
   \(\Rightarrow\)
     mng (EX (FnApp_sqpt (FVal fnid ftype NONE) args) se) s0
         (s0, ST (Block F pdecls [body]) 
                 (return_cont se rtype))
\end{alltt}
and where $\sigma$ is the same as $\sigma_0$ but with bindings
established between the values in $\mathit{args}$ and the formal
parameters in the function $f$.  The continuation accompanying the
expression at this point is simply the identity.  When the body
returns a value, that value will simply be substituted for what was
the function call, and expression evaluation can continue.  (The
\textsf{Ex} tag indicates that we are looking at an expression, while
\textsf{St} indicates that the value is a statement with its continuation.)

The next rule extends a continuation.  In the thesis semantics, we
have the rule
\[
\infer{\langle {\cal E}[e_0], \sigma_0\rangle \rightarrow_e
  \langle {\cal E}[e],\sigma\rangle}{
  \langle e_0,\sigma_0\rangle \rightarrow_e \langle e,\sigma\rangle}
\]
with $\cal E$ a context (a function from expressions to expressions).
In the new semantics we have
\[
\infer{\langle \textsf{Ex}({\cal E}[e_0], \mathit{se}_0), \sigma_0\rangle \rightarrow
  \langle \textsf{Ex}({\cal E}[e],\mathit{se}),\sigma\rangle}{
  \langle \textsf{Ex}(e_0,\mathit{se}_0),\sigma_0\rangle \rightarrow_e
  \langle \textsf{Ex}(e,\mathit{se}),\sigma\rangle}
\]
which is directly analogous.  But in addition, we also have
\[
\infer{\langle \textsf{Ex}({\cal E}[e_0],\mathit{se}_0),
  \sigma_0\rangle \rightarrow
  \langle \textsf{St}(s, {\cal E} \circ c), \sigma\rangle}{
  \langle \textsf{Ex}(e_0, \mathit{se}_0),\sigma_0\rangle
  \rightarrow
  \langle \textsf{St}(s, c), \sigma\rangle}
\]
which can be read as saying that if a nested expression evaluates to a
statement (i.e., a function call has begun), then the same function
call must be held to have begun at the outer level, but that the
continuation must insert the return value somewhat deeper into the
expression's syntax tree.  This is reflected by the (function)
composition of $\cal E$ with the continuation $c$ from the rule's
hypothesis.

These rules have specified what happens when an expression evaluation
switches to a statement evaluation.  In the opposite direction, when a
function call is about to return, the rule is
\[
\infer{%
  \langle\textsf{St}(\texttt{return}
  (\textsf{Ex}(\underline{(v,\tau)}, \mathit{se})), c), \sigma_0\rangle
  \rightarrow
  \langle \textsf{Ex}(c\underline{(v,\tau)}, \mathit{empty\_se}),
  \sigma\rangle}{
  \mbox{$\mathit{se}$ is empty of side effects}}
\]
where $\sigma$ is identical to $\sigma_0$ except that the various maps
for local variables, types and \texttt{struct}s have been reverted to
the state they held before the function was entered.

Note how the argument to the return statement is itself an extended
expression.  This means that the argument will be tagged with
\textsf{Ex} initially, but may later evolve to be a statement and
continuation (tagged with \textsf{St}) if the return-expression
includes a function call.  This means that the rule for evaluation of
the argument of return is
\[
\infer{\langle \textsf{St}(\texttt{return}(e_0),c), \sigma_0\rangle
  \rightarrow
  \langle \textsf{St}(\texttt{return}(e),c),\sigma\rangle}{
  \langle e_0, \sigma_0\rangle \rightarrow \langle e,\sigma\rangle}
\]
where $e_0$ and $e$ may be statements or expressions.

\subsection{Basic Object-Orientation}
\label{sec:basic-oo}

The inspiration for this part of the semantics is the article by
Wasserrab~\emph{et al}~\cite{wasserrab-nst-OOPSLA06}, which provides a
detailed model for multiple inheritance.  For the moment, my model
doesn't handle multiple inheritance, but I have nonetheless adopted
much of this article's basic technology as it certainly handles the
single-inheritance situation.

\subsubsection{Class Declaration}
\label{sec:class-declaration}
A class declaration is similar to the C model's declaration of a
\texttt{struct} type.  A class declaration takes two parameters, the
name of the class, and an optional ``class-info''
argument.\footnote{The information argument is optional to allow the
  situation where a forward declaration of a class occurs.}  The
class-info, if present, is a list of fields, coupled with an optional
ancestor class.  The latter allows single-inheritance.  The fields are
of two sorts, ``data'' fields or function definitions.  (A field
declaration of a function, though strictly not data, emulates the
situation where a member function is declared elsewhere.)  Both sorts
of fields are accompanied by a flag indicating whether or not they are
static, and a protection indicator (i.e., \texttt{public},
\texttt{protected}, or \texttt{private}).  For the moment, both
static-ness and protection information is entirely ignored, so that
all members are assumed to be \texttt{public} and non-static.

Member function definitions give the function's name, return-type,
parameter list (types and names), and function body.  This is a close
match for the abstract syntax.  All functions are assumed to be
virtual.

When a class declaration is encountered (only at the top-level;
neither nested or local classes are supported yet), its member
functions are entered into the state's function map.  The same
function map is used for normal (non-class) functions, so there is a
type of ``function-id'', declared
\begin{verbatim}
   fnid = GlobalFn of string | MFn of string => string
\end{verbatim}
meaning that such an identifier can be either global, in which case it
is given by one string (the function's name); or it can be a member
function, in which case it is given by a pair of strings: the class's
name and the function's name.

\subsubsection{Class Values and Dynamic Dispatch}
\label{sec:class-values}
There is no support for classes as r-values in the semantics as yet.
Instead, all expressions of class-type must be l-values.  This
restriction is based on the problems that will arise when multiple
inheritance is implemented.  In particular, with multiple inheritance
in place, it is no longer true that one can extract the byte sequence
for a given l-value by starting at the base address and taking as many
bytes from memory as the size of the type.  In particular, virtual
base classes may be at completely different places in memory, not
necessarily even contiguous with the rest of the object.  (This is
demonstrated for the \texttt{g++} compiler by the little program in
\texttt{notes/mult-inheritance-layout.cpp}.)

Absence of support for r-values means that, in this model, one can not
assign classes, pass them as parameters, or return them from
functions.  However, because references are supported (see below),
much idiomatic \cpp{} is still covered by the model.

The presence of classes means that the model's presentation of
l-values changes from the way it was in the original C model.  In
particular, an l-value that is statically typed as a base class needs
to know dynamically that it is really of a derived type.  This
information is traditionally recorded in \texttt{vtable} fields.
Following Wasserrab~\emph{et al}, my model instead records an
additional path accompanying every l-value.  This path is a list of
strings, listing the path through the ancestry-tree that leads from
the most-derived class to the current class type.

Consider, for example, the code in Figure~\ref{fig:oo-example}.  The
body of function \texttt{g} constructs the l-value \texttt{*b} when it
calls \texttt{f}.  As in the C model, this l-value will be associated
with some address, and the static type, which is \texttt{Class~B}.
But the additional path information that accompanies the l-value will
be the list \texttt{[D2,D1,B]}.  The last element of such paths is always
the static class type.  The prefix of the list makes it clear that
this is a \texttt{B} l-value that is dynamically a \texttt{D2}.  And
when field selection occurs, the \texttt{f} chosen will be
\texttt{D1}'s.

\begin{figure}[hbtp]
\begin{verbatim}
   class B {
     public: virtual int f() { return 3; }
   };

   class D1 : public B {
     public: int f() { return 4; }
   };

   class D2 : public D1 { };

   int g(class B *b) { return b->f(); }

   int main()
   {
     D2 d;
     return g(&d);
   }
\end{verbatim}
\caption{C++ code demonstrating OO-polymorphism.  The program will
  return 4. Though it appears as if \texttt{B}'s function \texttt{f}
  is called, the version of \texttt{f} called will actually be that
  attached to \texttt{D1}.}
\label{fig:oo-example}
\end{figure}

The rule encoding all this is \ruleid{function-member-select}:
\[
\infer{
  \begin{array}{l}
    \statebrack{\textsf{Ex}((\clvalue(a,\texttt{Class}\;C,p))\;\mathbf{.}\;
      \mathit{fld},\mathit{se}),\sigma} \rightarrow \\
    \qquad\left\langle\textsf{Ex}\left(\cfvalue\left(
        \begin{array}{l}
          \texttt{MFn}\;(\last(p'))\;\mathit{fld},\\
          (\funtype{\mathit{argtys}}{\mathit{ret}}),\\
          \SOME{(\clvalue\;a\;(\texttt{Class}\;(\last(p')))\;p')}
        \end{array}\right),
      \mathit{se}\right),\sigma\right\rangle
  \end{array}}{
  \sigma \vdash \hd(p) \textrm{ has least } \mathit{fld} :
  \funtype{\mathit{argtys}}{\mathit{ret}} \textrm{ via } p'}
\]

In the figure, the expression of interest is \texttt{b->f()}, which is
syntactic sugar for \texttt{((*b).f)()}.  (Note how the member
selection is syntactically subordinate to the function application.)
The \ruleid{function-member-select} rule governs the evaluation of
\texttt{(*b).f} once the left-hand-side (\texttt{*b}) has evaluated to
an l-value.\footnote{Even when the model allows for class r-values,
  they will be given a memory location (and thus, a life-time).  This
  will enable them to also be l-values.  In essence, it is not
  possible to create an object of any sort without giving it a
  location.  Contrast numbers, which can be ``created'', and not given
  a memory location, simply by writing them down.}

In our model, the l-value's address will be the same as the address of
its most-derived class.  In other words, the $a$ of the rule will be
the same as the address of the object \texttt{d}.  This is not what
happens in typical compilers, which will actually make the pointer to
the \texttt{B} sub-class point at the address of that sub-object's
fields in the wider object's memory layout.  Then, the fact that the
most-derived class is a \texttt{D2} is implicitly recorded in the
\texttt{vtable}, which will contain a pointer to \texttt{D1::f}.

In the rule above, the variable $C$ containing the name of the static
class will be \texttt{B}, and $p$, the path variable, will be
\texttt{[D2,D1,B]}.  The $\mathit{fld}$ variable will be \texttt{f}.
Then, the hypothesis will check the class hierarchy to determine where
an \texttt{f} can be found, starting at the head of the path, i.e., at
\texttt{D2}.  This will reveal that there is an \texttt{f} at path
\texttt{[D2,D1]}.  This will be the value for $p'$.

The RHS of the reduction arrow $\exprarrow$ features a new sort of
expression constructor, \cfvalue{}.  This is a ``function l-value'', a
designation of a particular function within the program.  The first
argument is the function identifier, as described earlier in
Section~\ref{sec:class-declaration}.  In this case, the function
identifier points at a class's member function given by two strings:
the name of the class where the function body is declared, and the
field-name.  In our example, the function identifier will be
\texttt{MFn~D1~f}, or simply \texttt{D1::f} in more traditional \cpp{}
notation.  The second argument of the \cfvalue{} constructor is the
type of the function.  The third argument is an optional expression
containing the l-value for the object associated with a member
function call.  (The use of $\SOME{\;}$ marks that this is an optional
value.)  This third argument is not present if the function is a
normal one (i.e., not a class-member).

There are two rules for performing function applications, but the only
difference between them is that the member function version sets up
the value of the \texttt{this} expression form.  Otherwise, both rules
extract the body for the given function-identifier, and set up
parameters in the same way.

\paragraph{Field Selection}
Field selection is also based on the notion of being able to find the
most-derived declaration of the given field in the ancestor hierarchy.
There is no need to worry about adjusting \texttt{this} pointers, or
performing analyses with dynamic types as field selections are always
done with respect to a class's static type.  However, there is an
additional complexity, stemming from the need to give addresses to
selected fields, so that they can become well-formed l-values.  In
turn, this relies on knowing how a class is laid out in memory.

The standard \emph{does} require that fields belonging to a particular
class type be laid out in the order in which they appear (as long as
they have the same access-specifiers, but I'm not modelling those just
yet).  But there is no specification of how base sub-objects are laid
out.  (Recall, moreover, that in the presence of a virtual base-class,
an object that is not most-derived may be split over distinct parts of
memory.)

With just single inheritance to model, I use an under-specified
function \texttt{class\_layout} which takes a set of sub-object paths,
and returns them in a list.  This then drives the calculation of field
offsets and class sizes.

\paragraph{What of vtables?}
The use of paths does away with the need for vtables.  On the other
hand, we wouldn't want to specify the model in such a way as to
preclude this perfectly reasonable implementation strategy.  In
particular, vtables will be catered for just as in the standard, by
maintaining that it is only in POD (``plain old data'') types where
one can rely on the address of the first field being the same as the
address of the containing \texttt{struct}.  The calculation of sizes
will also be under-specified to allow for the presence of extra,
user-invisible data at the start of a class.  (This latter hasn't been
done yet.)

\paragraph{An Axiom}
The mechanisation includes an axiomatisation stating that two
otherwise unspecified constants, \texttt{ptr\_encode} and
\texttt{ptr\_decode} are mutual inverses.  These partial functions map
between the dynamic arguments that constitute l-values (address and
path), and the lists of bytes that make up pointers.

\subsection{Reference Types}
\label{sec:reftypes}

Reference types pose problems in the contexts where they are
distinctive:
\begin{itemize}
\item passed as parameters to functions;
\item returned from functions;
\item when they are initialised.
\end{itemize}

Otherwise, it is obvious how the existing semantics should treat
references: they are l-values.  References are only created from
l-values, and in the reverse direction, values will turn into r-values
as required.  They will do the right things when of \texttt{class}
types because they will have the Wasserrab style paths attached.  In
other words, a reference to a base class may actually be an l-value
referring to a derived class.

The remaining sub-sections explain what happens in the three
interesting situations.

\paragraph{Omissions}
\begin{itemize}
\item The semantics does not handle references to functions.
  Supporting these will require more rules in the dynamic semantics,
  but these rules will be directly analogous to the rules presented
  below: wherever an \clvalue{} constructor (which is for l-values of
  object type) appears, there will need to also be a rule for
  \cfvalue{}, which is for function values.
\item The semantics does not handle initialisation of references
  belonging to classes because these have to happen in the
  \emph{mem-initialisors} of a class's constructor function(s), and
  constructor functions are not yet modelled.
\item The semantics doesn't handle initialisation of \ckey{const}
  references from r-values because it does not yet handle
  \ckey{const}.
\end{itemize}


\subsubsection{References as Parameters}

When a formal parameter is of reference type, the actual parameter
needs to stay an l-value rather than reduce to an r-value.  When the
called function's environment is being established in its local stack
frame, the binding for the formal name can be directly to the actual
l-value's address and type.  In other words, nothing is allocated in
memory to represent the reference.  This is not very likely in an
actual environment, which will probably have an address in memory
somewhere.  (The only way to detect this (and this would require
undefined behaviour) would be know where local variables were
allocated, and to scan this area byte-by-byte, presumably starting at
the address of some other, non-reference, local parameter.)

This requires a change to the existing semantics, to remove function
arguments from the $\cal L$ (l-value) context.  In order to allow some
function arguments to decay into r-values, a new rule
\ruleid{function-application-lval2rval} is introduced:
\[
\infer{
  \statebrack{\textsf{Ex}(f(\mathit{pfx} \frown e_0 ::
    \mathit{sfx}),
    \mathit{se}_0),s_0}
  \rightarrow
  \statebrack{\textsf{Ex}(f(\mathit{pfx}\frown e ::
    \mathit{sfx}),\mathit{se}),
    s}}{%
  \statebrack{\mathit{se}_0,e_0,s_0}\lvtorv\statebrack{\mathit{se},e,s} &
  f\mbox{ expects an r-value at position }|\mathit{pfx}|}
\]
The variables $\mathit{pfx}$ and $\mathit{sfx}$ are lists of
expressions corresponding to the other actual parameters being passed
to the function $f$.  The $\frown$ operator is list concatenation, and
$::$ is ``cons'', taking an element and putting onto the front of
another list ($::$ binds tighter than $\frown$).

The rule that determines that a function application's sequence point
has been reached (\ruleid{function-call-sqpt}) must also change.
Previously, this rule checked that the function and all of the
arguments had been fully evaluated, and being ``fully evaluated''
meant ``had been evaluated to a value (consisting of a list of
bytes)''.  The new rule checks that every parameter is either a
byte-list value (when the function doesn't expect a reference type),
or an l-value.

\subsubsection{References Returned from Functions}

This requires a number of changes to the semantics.  First, there
shouldn't be an \crvreq{} tag wrapped around the expressions that are
arguments to \texttt{return} statements.  Secondly, the continuation
that accompanies all statement evaluations (as of Deliverable-1),
needs to be adjusted so that it is one of two possibilities, a
continuation accepting a value, or a continuation accepting an
l-value.  The decision as to which sort of continuation it will be is
set up when a function call is made, because the desired return type
of the function is known at this point.

The new type of continuations is an algebraic type with two
constructors:
\begin{verbatim}
   conttype =
       RVC of (byte list -> CPP_Type -> CExpr)
     | LVC of (num -> CPP_Type -> string list -> CExpr)
\end{verbatim}
The rules for \texttt{return} are able to examine the type of
continuation directly.  So that \ruleid{return-lval2rval} is
\[
\infer{\statebrack{\textsf{St}(\ckey{return}\;\textsf{Ex}(e_0,\mathit{se}_0)\ckey{;},\;\ckey{RVC}(c)),s_0}\stmtarrow
  \statebrack{\textsf{St}(\ckey{return}\;\textsf{Ex}(e,\mathit{se})\ckey{;},\;\ckey{RVC}(c)),s}}{
  \statebrack{\mathit{se}_0,e_0,s_0}\lvtorv\statebrack{\mathit{se},e,s}}
\]
Every statement is accompanied by a continuation, the second argument
of the \textsf{St} tag. Also note that the first argument of a
\ckey{return} might itself be another statement.  If the original
expression contained a function call, the body of the called function,
a statement, would eventually become the top of the expression.  In
this rule, the use of the inner \textsf{Ex} tag precludes this
possibility.

There are two rules allowing a \ckey{return} statement to pass its
expression-value to the continuation.  The rule for returning normal
values is \ruleid{return-rvalue}:
\[
\infer{%
  \begin{array}[t]{l}
    \statebrack{\textsf{St}(\ckey{return}\;\textsf{Ex}(\cvalue{m}{\tau},\mathit{se})\ckey{;},\;\ckey{RVC}(c)),
      \;\sigma}%
    \;\;\rightarrow\\
    \qquad\left\langle\!\!\begin{array}{l}
      \textsf{Ex}(c\;m\;\tau,\mathit{empty\_se}),\\
      \sigma[\textsf{stack} := \mathit{stack}', \textsf{this} :=
      \mathit{this}, \Gamma := g, \Sigma := s, A
      := a]\end{array}\right\rangle
  \end{array}}{
  \mathit{se}\mbox{ is empty of side effects} & \textsf{stack}_\sigma = (s,g,a,\mathit{this})
  :: \mathit{stack}'}
\]
The \textsf{stack} and \textsf{this} fields of the state are not in
the original thesis semantics.  The \textsf{stack} field records the
stack of environment information.  The environment information
includes maps for variable typing ($\Gamma$), address information for
variables ($A$), class information ($\Sigma$) and the current
(l-)value for \ckey{this}, if there is one.   Use of the
\textsf{stack} is a necessary part of moving to a small-step semantics
for statements.

The rule for returning a reference \ruleid{return-lvalue} is similar:
\[
\infer{%
  \begin{array}[t]{l}
    \statebrack{\textsf{St}(\ckey{return}\;\textsf{Ex}(\clvalue(\mathit{ad},\tau,p),
      \mathit{se})\ckey{;},\;\ckey{LVC}(c)),
      \;\sigma}%
    \;\;\rightarrow\\
    \qquad\left\langle\!\!\begin{array}{l}
      \textsf{Ex}(c\;\mathit{ad}\;\tau\;p,\mathit{empty\_se}),\\
      \sigma[\textsf{stack} := \mathit{stack}', \textsf{this} :=
      \mathit{this}, \Gamma := g, \Sigma := s, A
      := a]\end{array}\right\rangle
  \end{array}}{
  \mathit{se}\mbox{ is empty of side effects} & \textsf{stack}_\sigma = (s,g,a,\mathit{this})
  :: \mathit{stack}'}
\]

\subsubsection{Declaring References}

When a reference is declared, it must also be initialised, and the
initialising expression must be an l-value.  But normal
initialisations need to be able to turn l-values into r-values so
there is an ``lvalue-to-rvalue'' rule for variable declarations that
are accompanied by initialisations.  This rule is
\ruleid{block-vdecinit-lval2rval}:
\[
\infer{\begin{array}{l}
    \statebrack{\textsf{St}(\mathbf{\{}
  (\textsf{VDecInit}\;\tau\;\mathit{name}\;(\textsf{Ex}(e_0,\mathit{se}_0))
  :: \mathit{vds})\;\mathit{sts} \mathbf{\}}, c),\sigma_0}
\rightarrow\\[2mm]
\qquad\statebrack{\textsf{St}(\mathbf{\{}
  (\textsf{VDecInit}\;\tau\;\mathit{name}\;(\textsf{Ex}(e,\mathit{se}))
  :: \mathit{vds})\;\mathit{sts} \mathbf{\}}, c)),\sigma}\end{array}}{
\tau \mbox{ not a reference type} &
\statebrack{\mathit{se}_0,e_0,\sigma_0}\lvtorv
  \statebrack{\mathit{se},e,\sigma}}
\]
where $\textsf{VDecInit}\;\tau\;\mathit{name}\;e$ is a value in the
abstract syntax type corresponding to the declaration of variable
$\mathit{name}$ with type $\tau$, to be initialised with expression
$e$.  The braces in the syntax above indicate that this happens in the
context of a statement-block.  (There are corresponding rules for
variable declarations and initialisations at the top-level of a
translation unit.)

A variable declaration of reference type is ready to ``fire'' when its
initialising expression has reduced to an l-value, and when there are
no remaining side effects in the side effect record.  At this point,
the variable address map in the state is updated to point at the
l-value's address, and the type map is also updated to make the type
of the name the same as the type of the l-value.





\subsection{Polymorphism \& Multiple Inheritance}
\label{sec:multiple-inheritance}

As suggested in earlier deliverables, multiple inheritance has been
modelled by following the approach described in Wasserrab \emph{et
  al}~\cite{wasserrab-nst-OOPSLA06}.  The changes supporting this
addition to the model are relatively minor and almost all appear in
the theory \texttt{class_info}, where a number of new definitions
appear.

\newcommand{\Cs}{\mathit{Cs}} \newcommand{\fld}{\mathit{fld}}

At the top level, the rule for calculating the function that will be
called dynamically (\ruleid{function-member-select}) has changed
slightly.  It now reads
\[
\infer{
  \begin{array}{l}
    \statebrack{\textsf{Ex}((\clvalue(a,\texttt{Class}\;C,\Cs))\;\mathbf{.}\;
      \fld,\mathit{se}),\sigma} \rightarrow \\
    \qquad\left\langle\textsf{Ex}\left(\cfvalue\left(
        \begin{array}{l}
          \texttt{MFn}\;(\last(\Cs'))\;\fld,\\
          (\funtype{\mathit{argtys}}{\mathit{dynret}}),\\
          \SOME{(\clvalue\;a\;(\texttt{Class}\;C)\;\Cs')}
        \end{array}\right),
      \mathit{se}\right),\sigma\right\rangle
  \end{array}}{
  \begin{array}{c}
    \sigma \vdash \last(\Cs) \textrm{ has least method } \fld :
    \funtype{\mathit{statargs}}{\mathit{statret}} \textrm{ via } Ds \\
    \sigma \vdash (C,\Cs \,@_p\, Ds) \textrm{ selects } \fld :
    \funtype{\mathit{dynargs}}{\mathit{dynret}} \textrm{ via } \Cs'
  \end{array}}
\]
This rule describes how the call to method $\fld$ is resolved
for an object located at address $a$, with dynamic type $C$, and where
the static type of the object is the last element of the list $\Cs$.
The list $\Cs$ is also a path through the class hierarchy, starting at
the dynamic type and ending at the current static type.  (Note that an
object's dynamic type is determined on object creation, and persists
for an object's entire life-time.  In contrast, an object's static
type is the type ascribed to it by a particular piece of code.
Different pieces of code may well ``see'' the same object as having
different types.  In this sense, an object's dynamic type is
unchanging, but it will have a variety of static types across the text
of a program.  Confusing, but true!)

The first premise (``has-least-method'') examines the static type of
the object for which the method will be called, $\last(\Cs)$.
Starting at that point in the hierarchy it looks upwards (\ie, towards
base classes) for the nearest base class that provides an
implementation of the desired method.  This base-class might be
$\last(\Cs)$ itself, in which case the path found ($Ds$) will be
empty.  There must also be a unique closest ancestor providing the
desired method.  If this isn't the case, then there will have been a
compile-time error, as the call will be statically ambiguous.

The second premise (``selects-via'') then determines the dynamic
location of the desired method.  There are two cases.  The simple case
is when there is a unique best method for the dynamic type, $C$.
Imagine, for example, that there is a four-element singly-linked
inheritance graph, from base $B_0$ down to most-derived $B_3$, with
implementations of the method $\fld$ at $B_0$ and $B_2$.  If the
object is actually of type $B_3$, but is statically seen as type
$B_1$, then $C$ is $B_3$, and $\Cs$ will be $[B_3,\,B_2,\,B_1]$.  The
first premise (``has-least-method'') determines that, starting at
$B_1$ (the static type) there is an implementation of $\fld$ at path
$[B_1, B_0]$.  This will be the instantiation of the rule's variable
$Ds$.

The simple case for ``selects-via'' checks whether or not there is
also a least method for the dynamic type (ignoring, for the moment,
the path giving the static type).  Thus
\[
\infer{\sigma \vdash (C,\Cs) \textrm{ selects } \fld \textrm{ via } \Cs'}{\sigma \vdash C \textrm{ has least method } \fld :
  \funtype{\mathit{argtys}}{\mathit{retty}} \ \textrm{ via } \Cs'}
\]
In the simple example, we will thus conclude that \[
\sigma \vdash (B_3,[B_3,B_2,B_1,B_0]) \textrm{ selects } \fld
\textrm{ via } [B_3,B_2]
\]
so that the call will be to the implementation of $\fld$ in $B_2$, and
the \texttt{this} pointer will be adjusted so that the type and path
information associated with its value will be $(B_3,[B_3,B_2])$ (a
dynamic type of $B_3$, as always, and a static type of $B_2$).

The same rule applies in a much more complicatd seeming situation,
where multiple inheritance and shared base objects come into play.
Consider the program in Figure~\ref{fig:diamond-cpp}.  When the call
to \texttt{cref.f()} is made, the type and path associated with the
reference will be $(D,[D,C_1])$.  Statically, the reference is a
\texttt{C1} value, but dynamically, it's really of class
\texttt{D}. The first premise in rule \ruleid{function-member-select}
finds that there is a path ($\mathit{Ds}$ in the rule) which is
appropriate for \texttt{f}.  This path is $[B]$.  The fact that the
path does not include the derived object's name, and is just a bare
reference to a class indicates that it is a path to a shared base.
When such a path is the second argument of path concatenation (the
$@_p$ operator in the second premise), the result is just the second
argument, so that the second hypothesis in the rule becomes
\[
\sigma \vdash (D,[B]) \textrm{ selects } f :
\funtype{\texttt{void}}{\texttt{int}} \textrm{ via } \Cs'
\]
\begin{figure}[hbtp]
\begin{verbatim}
#include <iostream>

class B {
public:
  virtual int f() { std::cout << "B's f\n"; return 3; }
  virtual ~B() { }
};

class C1 : virtual public B {
};

class C2 : virtual public B {
public:
  virtual int f() { std::cout << "C2's f\n"; return 4; }
};

class D : public C1, public C2 {
};

int dosomething(C1 &cref)
{
  return cref.f();
}

int main()
{
  D d;
  return dosomething(d);
}
\end{verbatim}
\label{fig:diamond-cpp}
\caption{Multiple inheritance with shared base objects.  (This program is
in the \texttt{notes} directory with name \texttt{diamond-multinherit.cpp}.)}
\end{figure}

The first, simple, rule for ``selects-via'' resolves this.  Ignoring
the path $[B]$, the simple rule checks whether or not there is a
unique least path to an \texttt{f} from $D$.  There is such a path,
and it is $[D,C_2]$.  So, the call to \texttt{cref.f()} ends up being
a call to the \texttt{f} in class $C_2$, in an ``unrelated'' part of
the object hierarchy.
\[
\rule{0.2\textwidth}{.1mm}
\]

For those situations where there is not a unique path from the dynamic
type to a best selection, there is another more complicated rule
defining ``selects-via''.  In Figure~\ref{fig:lopsided-v}, the
inheritance hierarchy looks like
\[
\psset{rowsep=3ex,nodesep=2mm,colsep=2em}
\begin{psmatrix}
\texttt{\psframebox{B}} \\
\texttt{Left1}\\
\texttt{\psframebox{Left2}} & & \texttt{\psframebox{Right}}\\
& \texttt{D}
\end{psmatrix}
\ncline{1,1}{2,1}
\ncline{2,1}{3,1}
\ncline{3,1}{4,2}
\ncline{3,3}{4,2}
\]
where the boxed class names are those that implement the function
\texttt{f}.  The question is which \texttt{f} will be called when the
dynamic type is \texttt{D}, and when the static type is
\texttt{Left1}.  There is no unique, least implementation of
\texttt{f} visible from the dynamic type (\texttt{D}), so the simple
rule does not apply.  Instead, the notion of \emph{overrider} is
introduced via the rule
\[
\infer{\sigma\vdash (C,\Cs) \textrm{ selects }\fld : m \textrm{ via }
  \Cs'}{
\begin{array}{c}
  \forall m\;\Cs'. \;\;\neg \;\sigma \vdash C \textrm{ has least }\fld : m
\textrm{ via } \Cs'\\
\sigma \vdash (C,\Cs) \textrm{ has overrider } \fld : m \textrm{ via }
\Cs'
\end{array}}
\]
where the static type of the value again plays a role.  In this case,
the \texttt{f} that gets called is that in \texttt{Left2}.  For
further details on exactly how this is defined, see either
\texttt{class_infoScript}, or~\cite{wasserrab-nst-OOPSLA06}.

\begin{figure}[hbtp]
\begin{verbatim}
#include <iostream>

class B {
public:
  virtual void f() { std::cout << "B's f\n"; }
  virtual ~B() { }
};

class Left1 : public B { };

class Left2 : public Left1 {
public:
  virtual void f() { std::cout << "Left2's f\n";  }
};

class Right {
public:
  virtual void f() { std::cout << "Right's f\n"; }
  virtual ~Right() { }
};

class D : public Left2, public Right { };

void dosomething(Left1 &l1ref) { l1ref.f(); }

int main()
{
  D d;
  // d.f();  would be statically ambiguous
  dosomething(d);
  return 0;
}
\end{verbatim}
\caption{A lop-sided `V' inheritance hierarchy.  Available as
  \texttt{notes/lopsided-v.cpp}.}
\label{fig:lopsided-v}
\end{figure}

\subsection{Object Lifetimes}
\label{sec:object-lifetimes}

\paragraph{Constructors}
Handling constructors has easily wrought the greatest change to the
previous deliverable's model.  The basic approach taken to objects has
been to encode as much as possible with ``evolving syntax''.  Just as
\texttt{while} can be modelled by having
\[
\texttt{while}\;\texttt{(}G\texttt{)}\;\mathit{body}
\] become \[
\texttt{if}\;\texttt{(}G\texttt{)}\;\texttt{\{}\mathit{body}\texttt{;}\;\texttt{while}\;\texttt{(}G\texttt{)}\;\mathit{body} \texttt{\}}
\]
so too do calls to \cpp{} constructors create new programs ``in
place''.  The advantage of this approach is that there is less need
for relatively complicated state to be recorded in yet more fields in
the big record type \texttt{state} (in \texttt{statesScript}).
Instead, programs can unfold into more elaborate forms directly.  The
disadvantage of this approach is that the original syntax may not
support enough forms, requiring new constructors to be created, or
extended with new parameters.

The existing handling of block-statements is an example of this latter
disadvantage.  In particular, the abstract syntax has a constructor
\texttt{Block} with type
\[
\texttt{:bool -> decl list -> stmt list}
\]
where the boolean flag indicates whether or not the block has been
entered yet.  In this way, the abstract syntax has values in it that
can't be written down in the concrete syntax.  (Simiarly, the original
\textsf{Cholera} model for C included the \texttt{RVR} constructor and
the $\hat{f}$ intermediate form for function calls.)

Constructors are intimately tied up with declarations and
initialization.  In the simple C world, a variable comes into being in
two stages.  First space in memory is allocated for the variable
(whether on the stack or heap), and the variable name is associated
with that space for the span of the variable's life.  Then there is an
optional initialization phase, when the piece of memory associated
with the variable space is filled in with some value.

The \cpp{} model is similar.  All new objects (but not references)
must be associated with some new space.  Then they may or may not be
initialised.  Additionally, in \cpp{}, an object that is only
declared, and which does not appear to be initialised, will actually
have its default constructor called.

The abstract syntax supporting this is all defined in the HOL source
file \texttt{statementsScript}:

\begin{center}
\begin{tabular}{lll}
Constructor & Argument Types & Description\\
\hline
\texttt{VDec} & $\mathit{name},\mathit{type}$ & no initialization\\
\texttt{VDecInit} &
$\mathit{name},\mathit{type},\mathit{initializer}$ &
initialization (unallocated) \\
\texttt{VDecInitA} &
$\mathit{varlocation},\mathit{type},\mathit{initializer}$ &
initialization (space allocated)
\end{tabular}
\end{center}

For example, when a class is declared with no explicit initialization
of any sort, meaning that the default constructor will be called, the
syntax moves through all three stages.  The abstract syntax
corresponding to something like
\begin{verbatim}
{
   classname c;
   ...
}
\end{verbatim}
will be \texttt{VDec "c" classname}.  Because this is a class type,
rule \ruleid{decl-vdec-class} will fire (and can do so immediately),
and the declaration will become \texttt{VDecInit "c" classname init},
where the form of \texttt{init} will encode the fact that a direct
initialization is being performed, and that there are no arguments.
Then the rule \ruleid{decl-vdecinit-start-evaluate-direct-class}
fires.  Side conditions of this rule cause space to be allocated (at
address \texttt{a}), the state's maps from names to addresses be
updated, and for the construction of the class to be recorded so that
it can be destroyed later.

The syntax also evolves to become
\begin{verbatim}
   VDecInitA classname (ObjPlace a) init'
\end{verbatim}
where the new initializer records that a function call to a
constructor is about to happen, and where that construction will
happen (\ie, \texttt{init'} includes a reference to \texttt{a}).

There are three forms of initializer.  The first two are
\texttt{DirectInit0} and \texttt{DirectInit} and correspond to direct
initialization~\cite[\S 8.5 paragraph 12]{cpp-standard-iso14882}.  The
\texttt{DirectInit0} constructor takes as arguments a list of
expressions.  In this way, concrete syntax such as
\begin{verbatim}
{
  Classname c(x,&y,z+1);
}
\end{verbatim}
is directly modelled.  (When the rule \ruleid{decl-vdec-class} fires,
the newly created \texttt{DirectInit0} constructor takes an empty list
of arguments.)

The \texttt{DirectInit} constructor takes one argument, an ``extended
expression'', which will initially be an expression constructed by an
application of the special constructor \texttt{ConstructorFVal} to the
same argument list.  This form needs to be an extended expression so
that the body of the constructor (a statement) can be entered.

The other form of initializer is constructed by the function
\texttt{CopyInit}.  This corresponds to the syntax
\begin{verbatim}
{
  type varname = expression;
  ...
}
\end{verbatim}
which is a
copy-initialization~\cite[\emph{ibid}]{cpp-standard-iso14882}.  When
the type of the new object is not a class, there is no difference
between copy-initialization and direct-initialization, reflected in
the rule \ruleid{decl-vdecinit-start-evaluate-direct-nonclass}, which
moves from a \texttt{DirectInit0} to a \texttt{CopyInit} initializer.

When a \texttt{CopyInit} initializer completes its evaluation,
yielding a value, that value be copied across into the space earlier
allocated for the object.  For non-class types this is done with the
same \texttt{val2mem} helper function that is used to apply side
effects.  For class types, this copying must be performed by a call to
the copy constructor.

\paragraph{Constructor Calls}
The expression form corresponding to a constructor call uses the
expression form for function applications, but with a special form in
the place of the function value.  This allows the normal evaluation of
function applications (with the unspecified order of evaluation of
arguments, for example).  The function value position is filled by a
new abstract syntactic form \texttt{ConstructorFVal}.  This takes
three parameters:
\begin{itemize}
\item a boolean indicating whether or not the constructor is being
  called for a most-derived object or not;
\item the address of the space which the constructor is to operate
  over; and
\item the name of the class that is being constructed
\end{itemize}
So, if a declaration is made of the form
\begin{verbatim}
{
   class v(x);
}
\end{verbatim}
then, once sufficient space is allocated for the new object \texttt{v}
(at address \texttt{a}, say), the abstract syntax will look like
\begin{verbatim}
  VDecInitA "class"
            (ObjPlace a)
            (DirectInit
               (NormE (FnApp (ConstructorFVal T a "class")
                             [Var "x"])
                      base_se))
\end{verbatim}
The \texttt{ObjPlace} constructor is used to distinguish this from the
situation where a reference is being initialised.  The \texttt{NormE}
constructor specifies that the current form is an expression (as
opposed to the statement that will be in this position once the
constructor body is entered).  The \texttt{base_se} value is the empty
side-effect record.  This will evolve as references to and updates of
memory occur.

Once the parameters to the constructor have been evaluated, the
constructor body can be entered.  This happens in rule
\ruleid{constructor-function-call}:
\[
\infer{
  \begin{array}{c}
    \statebrack{\textsf{Ex}(\texttt{FnApp_sqpt}\;
      (\texttt{ConstructorFVal}\;\mathit{mdp}\;\mathit{addr}\;C)\; \vec{a},
      \mathit{se}_0),\sigma_0)}\\
    \longrightarrow\\
    \left\langle
      \begin{array}{l}
        \textsf{St}\left(
          \begin{array}{l}
            \texttt{Block}\;\bot\;(\mathit{pdecls}\frown\mathit{cpfx})\;b,\\
            \texttt{return_cont}\;\mathit{se}_0\;\texttt{Void}
          \end{array}\right),\\
        \sigma_0 \textrm{ with }
        \texttt{thisvalue := }\lfloor\underline{(\mathit{this},(C\texttt{*}))}\rfloor
      \end{array}
    \right\rangle
  \end{array}}{
  \begin{array}{l}
    \texttt{find_constructor_info}\;\sigma_0\;C\;\vec{a}\;\vec{p}\;\vec{\mathit{m\imath}}\;b\\
    \!\!\begin{array}{l}
      \mathit{pdecls} =\\
      \quad\textsf{map}
      \begin{array}[t]{l}
        (\lambda((n,\tau),a).\;
        \texttt{VDecInit}\;\tau\;n\;(\texttt{CopyInit}(\textsf{Ex}(a,\texttt{base_se})))) \\
        (\textsf{zip}(\vec{p},\vec{a}))
      \end{array}
    \end{array}\\
    \lfloor \mathit{this}\rfloor =
    \texttt{ptr_encode}\;\sigma_0\;\mathit{addr}\;C\;[C]\\
    \mathit{cpfx} = \texttt{construct_ctor_pfx}\;\sigma_0\;\mathit{mdp}\;\mathit{addr}\;C\;\vec{\mathit{m\imath}}
  \end{array}}
\]
This is complicated enough, and there is more complexity hidden behind
the auxiliary functions and relations.  The first auxiliary is the
relation \texttt{find_constructor_info}, which appears in the rule's
first hypothesis.  This relation treats its first three parameters
($\sigma_0$, $C$ and $\vec{a}$) as inputs.  These are the current
state, the name of the class being constructed, and the actual
arguments being passed to the constructor.  The remaining arguments to
the relation are ``outputs''.  The vector $\vec{p}$ is the list of
formal parameters (names and types).  The vector
$\vec{\mathit{m\imath}}$ is the list of ``mem-initializers''
(see~\cite[\S12.6.2]{cpp-standard-iso14882}) associated with the
constructor, and $b$ is the constructor's body.  The
\texttt{find_constructor_info} auxiliary is responsible for resolving
which constructor needs to be called, based on the types of the actual
arguments.

The second hypothesis of the rule constructs the sequence of variable
declarations corresponding to the parameters, using standard
functional programming auxiliaries \textsf{map} and \textsf{zip}.
Parameter passing is just like variable declaration\footnote{This does
  away with the \textsf{Cholera} approach which had a number of
  auxiliary relations effectively duplicating what occurred in
  variable declaration}, and so the model's existing treatment of
declarations can be re-used to set up the binding between formal names
and actual values.  Note that the expressions that initialise the
parameters have already been fully evaluated, so that there will be no
expression evaluation done when the declarations come to be evaluated
(except for any class construction that may be called for).

The third hypothesis calculates a value for the \texttt{this} value.
The dynamic and static type of the \texttt{this} pointer will be the
same (as the pair $C$ and $[C]$ are passed to \texttt{ptr_encode}),
and there will not be any polymorphic dispatch to functions in derived
classes if any virtual functions are called in the constructor bodies.

The fourth hypothesis constructs a ``c-prefix'' of declaration calls
to initialise class members and bases.  This is all done in the
complicated function \texttt{construct_ctor_pfx} (defined in
\texttt{dynamicsScript}).  This constructs a sequence of declarations
to initialise the non-static members of the new class, and the class's
immediate bases.  The mem-initializers are consulted to see what
initializers should be provided.  (If a mem-initializer is not
provided for a given member or base, then that object will be value-
or default-initialized; see~\cite[\S12.6.2, paragraphs
3--4]{cpp-standard-iso14882}.)

For example, in Figure~\ref{fig:mem-inits}, before class \texttt{C}'s
constructor body is even entered, the parameters \texttt{cptr} and
\texttt{i} need to be declared and initialised with actual values.
Subsequently, all of \texttt{C}'s immediate bases (just \texttt{B} in
this case) need to be constructed, followed by its members
(\texttt{ptr} and \texttt{sz}).  Note that while the paramters need to
have space allocated for them, the bases and members do not (because
the space for the entire object was allocated at the \texttt{VDecInit}
stage).

\begin{figure}[htbp]
\begin{verbatim}
#include <cstring>

class B {
  int x;
public:
  B(int i) : x(i) {}
};

class C : public B {
  char *ptr;
  int sz;
public:
  C(char *cptr, int i)
    : B(cptr[i]), ptr(cptr), sz(strlen(cptr)) { }
};
\end{verbatim}
\caption{\cpp{} constructors with \emph{mem-initializers}.  (Available
as \texttt{notes/mem-inits.cpp}.)}
\label{fig:mem-inits}
\end{figure}

Assume that the constructor has been called with parameters \texttt{x}
and \texttt{y}, and that these have evaluated to values \texttt{xval}
and \texttt{yval}. The sequence of declarations that are constructed
to precede the constructor body is given in
Figure~\ref{fig:constructor-vdecs}. The first two declarations are of
the parameters.  The next constructs the base \texttt{B}, and the last
two construct the non-static members.  The body of
\texttt{construct_ctor_pfx} is responsible for calculating the offsets
of the members (given as \texttt{Boff}, \texttt{ptroff} and
\texttt{szoff} in the figure.

\begin{figure}[hbtp]
\begin{verbatim}
  VDecInit (char *) cptr (CopyInit (NormE xval base_se))
  VDecInit int      i    (CopyInit (NormE yval base_se))

  VDecInitA B (ObjPlace (a + Boff))
              (DirectInit
                 (NormE
                    (FnApp (ConstructorFVal F (a + Boff) B)
                           [Deref(Plus (Var "cptr")
                                       (Var "i"))])
                    base_se))

  VDecInitA (char *) (ObjPlace (a + ptroff))
                     (CopyInit (NormE (Var "cptr") base_se))
  VDecInitA int      (ObjPlace (a + szoff))
                     (CopyInit (NormE (FnApp (Var "strlen")
                                             [Var "cptr"])
                                      base_se))
\end{verbatim}
\caption{The variable declarations constructed to precede the body of
  C's constructor (from Figure~\ref{fig:mem-inits}).}
\label{fig:constructor-vdecs}
\end{figure}

Note how the first argument of the \texttt{ConstructorFVal} form in
the construction of the base \texttt{B} is false; this is because
\texttt{B} is not the most-derived object.  If there were any shared
bases in the example, the most-derived object would be ``responsible''
for constructing them (see~\cite[\S12.6.2, paragraph
5]{cpp-standard-iso14882}).  If \texttt{C} had any non-static members
of class type, then these would be constructed with their
$\mathit{mdp}$ flag set to true.

When the constructor for the base class \texttt{B} comes to be called,
it will in turn initialise its members.  The constructor for
\texttt{B} is called with an argument (\texttt{cptr[i]}) that needs to
be evaluated in the context where the parameters are in scope, so it
is clear that the declarations for the parameters must come before the
base and member initialisations.

\paragraph{Object Destruction}
When an object of class type is first declared (with a
\texttt{VDecInit} form), it has memory allocated sufficient to contain
the new class in its entirety (including sub-objects).  This
allocation is reflected in the state's \texttt{allocmap}.  When the
block in which this declaration was made is left, this allocation is
forgotten.  At the same time, the destructor for the object must be
called.  This is modelled with a new field for the state,
\texttt{blockclasses}.  This is of type
\begin{verbatim}
   (addr # CPPname # CPPname list) list list
\end{verbatim}
The address is the address of the object which is to be destroyed, and
the two remaining are the class name and path which serve to identify
the object's type.  The inner list constructor reflects the fact that
multiple objects may be declared at the same scope level.  The outer
list constructor appears because of the stack discipline necessary for
multiple nested scopes.

When an object of class type is declared, the rule responsible
(\ruleid{decl-vdecinit-start-evaluate-direct-class}) both allocates
the necessary memory, and adds address and type information for all of
the object's sub-classes and members (not just immediate bases, but
doing a complete traversal of the inheritance graph) to the
\texttt{blockclasses} field.  This is done by the
\texttt{update_blockclasses} relation defined in
\texttt{class_infoScript}.  The object information has to be added
to the list in the correct order so that objects will be destroyed in
reverse order of construction.

The rule for exitting from a block is then adjusted so that it can
only occur if the current scope's \texttt{blockclasses} information is
empty.  As long as it is not empty, the destructor corresponding to
the object on the top of the stack is set up to be called.  This is
done in rule \ruleid{block-exit-destructor-call}:
\[
\infer{
  \begin{array}{c}
    \statebrack{
      \textsf{St}(\texttt{Block}\;\top\;[]\;[\mathit{st}],c),\sigma}\\
    \longrightarrow\\
    % resulting state
    \left\langle
      \begin{array}{l}
        \textsf{St}\left(\texttt{Block}\;\top\;[]\;
          \left[
            \begin{array}{l}
              \texttt{Alone}
              (\textsf{Ex}(\texttt{DestCall}\;a\;\mathit{cnm}),\texttt{base_se}),\\
              \mathit{st}
            \end{array}\right],
          c\right),\\
        \sigma\textrm{ with }\texttt{blockclasses := }
        \mathit{rest} :: \mathit{bcs}\\
      \end{array}
    \right\rangle % end of resulting state
  \end{array}
  }{\texttt{final_stmt}\;\mathit{st} &
    \sigma.\texttt{blockclasses} = ((a,\!\mathit{cnm},p) ::
    \mathit{rest}) :: \mathit{bcs}}
\]
This is another example of evolving syntax: the block that the flow of
control is about to leave, has this departure deferred with the
insertion of a new statement before the block's final statement.  The
rule allowing an exit to eventually occur is \ruleid{block-exit}:
\[
\infer{
  \statebrack{
    \textsf{St}(\texttt{Block}\;\top\;[]\;[\mathit{st}], c), \sigma_0}
  \longrightarrow
  \statebrack{\textsf{St}(\mathit{st}, c), \sigma}
}{
  \begin{array}{l}
    \sigma_0.\texttt{stack} =
    (\mathit{stm},\mathit{tym},\mathit{vrm},\mathit{this}) ::
    \mathit{stk}
    \\
    \texttt{final_stmt}\;\mathit{st}
    \\
    \sigma_0.\texttt{blockclasses} = [] :: \mathit{bcs}
    \\
    \sigma = \sigma_0 \textrm{ with } \left\langle\begin{array}{ll}
        \texttt{stack := }\mathit{stk}; &
        \texttt{classmap := }\mathit{stm};\\
        \texttt{typemap := }\mathit{tym}; &
        \texttt{varmap := }\mathit{vrm};\\
        \texttt{thisvalue := }\mathit{this}; &
        \texttt{blockclasses := }\mathit{bcs}
      \end{array}\right\rangle
  \end{array}
}
\]
The predicate \texttt{final_stmt} is true of a statement if it is
an abnormal exit form (such as \texttt{break} or \texttt{return}), or
simply \texttt{EmptyStmt}.

Actually calling a destructor is straightforward because there are no
parameters, nor anything comparable to the mem-initializers.  The
requirement in the standard that sub-objects be destroyed before the
body of a destructor is entered is handled by the fact that the
sub-object information is entered into the \texttt{blockclasses} stack
ahead of the final encompassing object.

\subsection{Exceptions}
\label{sec:exceptions}

Exceptions are modelled in a way similar to the treatment of
\texttt{return}, \texttt{break} and \texttt{continue}.  One difference
is that exceptions propagate further: the \texttt{return} ``value''
only propagates up as far as a function call (within an expression).
In contrast, an exception will continue to propagate up through the
call-stack until it hits a suitable handler.

\newcommand{\ethrow}{\texttt{throw}\ensuremath{_e}}
This much allows a preliminary sketch of the behaviour.  The
\texttt{throw} form is actually an expression, but we set things up so
that there is a statement-level version of \texttt{throw} as well, and
it will be this that propagates through statement syntax.  The
expression syntax is written \ethrow{}, and the rule
\ruleid{expression-throw-some} describes the behaviour when the
\texttt{throw} has an argument:
\[
\infer{
  \statebrack{\textsf{Ex}(\ethrow(e)), \sigma}
  \to
  \statebrack{\textsf{St}(\texttt{throw}(e),c), \sigma}}{}
\]
The variable $c$ represents the continuation that would normally
convert the result of the statement into a value to be inserted into a
containing expression tree.  Because \texttt{throw} values can't ever
turn into values until they initialise a handler, this $c$ can be
anything at all.

At the statement level, the \texttt{throw} form takes an extended
expression as an argument.  This evaluates its argument as one might
expect (rule \ruleid{throw-expr-evaluation})
\[
\infer{\statebrack{\texttt{throw}\;e_0,\sigma_0}\stmtarrow\statebrack{\texttt{throw}\;e,\sigma}}{\statebrack{e_0,\sigma_0}\to\statebrack{e,\sigma}}
\]
When a \texttt{throw}'s expression has been completely evaluated, we
have something that can then propagate upwards through the abstract
syntax of statements.  Because of the way the rules for loops and
\texttt{if}-statements work, their sub-statements are never executed
while still sub-statements.

We start by specifying how \texttt{throw}-statements traverse
\texttt{Block} values.  This propagation uses the rule
\ruleid{block-interrupted}:
\[
\infer{
  \statebrack{
    \mathbf{\{} \;[\,] \;(s\!::\!\mathit{sts})\,\mathbf{\}},
    \sigma
  }
  \stmtarrow
  \statebrack{\mathbf{\{} \;[\,]\; [s]\mathbf{\}},\sigma}}{
  \textsf{final\_stmt}(s) & s \neq \textsf{EmptyStmt} & \mathit{sts}
  \neq [\,]}
\]
That this is a block rule is indicated by the
$\mathbf{\{\,\}}$-delimiters.  The first argument inside the block on
both sides of the arrow is an empty list of declarations.  The
predicate \textsf{final\_stmt} is true of \texttt{throw} and
\texttt{return} statements with fully evaluated arguments, as well as
of \texttt{break}, \texttt{continue} and the \textsf{EmptyStmt} form.
The latter doesn't cause an interruption, so is excluded by the second
hypothesis to the rule.  The final hypothesis ensures that there isn't
an infinite loop on this rule.

A \naive{} version of the rule for exiting a block
(\ruleid{block-exit}) is
\[
\infer{
  \statebrack{\mathbf{\{}\;[\,]\;[s]\;\mathbf{\}},\sigma}
  \stmtarrow
  \statebrack{s,\sigma}
}{
  \textsf{final\_stmt}(s)
}
\]
If a block has a final statement as its only statement, then that
statement can be propagated out to the top-level of the abstract
syntax. We will return to this rule when we come to worry about the
interaction of exceptions and object lifetimes.

There is also rule \ruleid{trap-exn-passes} for exception statements
escaping the \texttt{Trap} form (which is used for handling
\texttt{continue} and \texttt{break}):
\[
\infer{
  \statebrack{
    \textsf{St}(\texttt{Trap}\;\mathit{tt}\;\mathit{exn}, c), \sigma
  }
  \to
  \statebrack{\textsf{St}(\mathit{exn},c), \sigma}
}{\textsf{is\_exnval}(\textsf{St}(\mathit{exn},c))}
\]

\medskip
Because exceptions arise from expressions, the statement level rules
need to acknowledge this possibility.  Thus, this rule for
\texttt{if} (\ruleid{if-exception}):
\[
\infer{
  \statebrack{\textsf{St}(\texttt{if}\;G\;t\;e, c), \sigma}
  \to
  \statebrack{\textsf{mk\_exn}(G,c),\sigma}
}{
  \textsf{is\_exnval(G)}
}
\]
where $\textsf{is\_exnval}(G)$ is true if $G$ is of the form
$\textsf{St}(\texttt{throw}(e), c_0)$, and $e$ has been fully
evaluated.  The function \textsf{mk\_exn} takes an exception value and
replaces its continuation information with something appropriate for
the level of the containing statement.  In this example, $c_0$ will be
replaced by $c$.  There is a similar rule for the standalone
expression and \texttt{return} forms, as well as for the
statement-level \texttt{throw} form itself.  (The rule
\ruleid{expression-throw-some} turns a \ethrow{} into a \texttt{throw}
statement immediately, without evaluating the argument.  When the
argument \emph{is} evaluated, it may itself cause an exception.)

Because exceptions can arise in variable declarations, there is also a
rule for handling these.  This is \ruleid{block-declmng-exception}:
\[
\infer{
  \statebrack{
    \textsf{St}(\mathbf{\{}\,(d\!::\!\mathit{ds})\;sts\,\mathbf{\}}, c),
    \sigma_0
  }
  \to
  \statebrack{
    \textsf{St}(\mathbf{\{}\,[\,]\;[\texttt{throw}(\mathit{ex})]\,\mathbf{\}}, c),
    \sigma
  }
}{
  \statebrack{d,\sigma_0}\to_d
  \statebrack{[\texttt{VDecInitA}\;\tau\;\mathit{loc}\;e], \sigma}
  &
  e = \textsf{St}(\texttt{throw}(\mathit{ex}), c')
}
\]
Again, note how the continuation initially associated with the
exception ($c'$) is ignored.


\subsubsection{Handling Exceptions}

Handling exceptions is done with the \texttt{try-catch} form, which is a
sequence of handlers associated with a statement that might raise an
exception.  In the concrete syntax, programmers write something like
\newcommand{\suplus}{\ensuremath{^+}}
\newcommand{\sustar}{\ensuremath{^*}}
\begin{alltt}
   try \{
     \emph{statement}\sustar
   \}
   \emph{handler}\suplus
\end{alltt}
where a \emph{handler} is of the form
\begin{alltt}
   catch (\emph{guard}) \{ \emph{statement}\sustar \}
\end{alltt}
and a \emph{guard} can be ``\texttt{...}'' (\ie, three full-stops), a
type, or a standard parameter declaration (associating a type with a
name).

At the abstract syntax level, this is captured by the following HOL
declarations (in \texttt{statementsScript}):
\begin{alltt}
   exn_pdecl = (string option # CPP_Type) option

   stmt = ...
        | Catch of stmt => (exn_pdecl # stmt) list
\end{alltt}
In what is to come, I will write $*$ to correspond to the ``null''
value in option types, and use $\lfloor x\rfloor$ to make it explicit
that $x$ is in an option type.

\bigskip
\noindent
Statements can evaluate as usual under a \texttt{Catch}
\ruleid{catch-stmt-evaluation}:
\[
\infer{
  \statebrack{\textsf{St}(\texttt{Catch}\;s\;\mathit{hnds}, c),
    \sigma_0}
  \to
  \statebrack{\textsf{St}(\texttt{Catch}\;s'\;\mathit{hnds}, c),
    \sigma}
}{
  \statebrack{\textsf{St}(s,c),\sigma_0}\to
  \statebrack{\textsf{St}(s',c),\sigma}
}
\]
Non-exception statements pass through \texttt{Catch} statements,
ignoring the handlers \ruleid{catch-normal-stmt-passes}:
\[
\infer{
  \statebrack{
    \textsf{St}(\texttt{Catch}\;s\;\mathit{hnds}, c),
    \sigma
  }
  \to
  \statebrack{\textsf{St}(s,c), \sigma}
}{
  \textsf{final\_stmt}(s) &
  \lnot\textsf{is\_exnval}(\textsf{St}(s,c))
}
\]

There are three rules governing how handlers interact with thrown
exceptions.  The first describes the behaviour when the handler
parameter is given as ``\texttt{...}'' \ruleid{catch-all}:
\[
\infer{
  \begin{array}{l}
    \statebrack{
      \textsf{St}(\texttt{Catch}\;(\texttt{throw}(e))\;((*,
      \mathit{hnd})::\mathit{hnds}), c), \sigma
    }
    \to\\[1ex]
    \quad \statebrack{
      \textsf{St}(\mathbf{\{}\,[\,]\;[\mathit{hnd};\,\textsf{ClearExn}]\,\mathbf{\}}, c),
      \sigma[\textsf{current\_exns}\,:=\,e::\textsf{current\_exns}\,]
    }
  \end{array}
}{
}
\]
This rule introduces two new features, the \textsf{current\_exns}
field of the program state, and the \textsf{ClearExn} statement-form.
Both are present to support the ability of programs to use
\texttt{throw} without an argument to re-throw the ``current
exception''.  This is covered below in Section~\ref{sec:throw-none}.

Otherwise, the behaviour is clear: if the top handler has
``\texttt{...}'' as its parameter, then this handler is entered (and
the other handlers are discarded).

When the top handler has an explicitly-typed parameter, the handler is
only entered if the type of the thrown value matches
\ruleid{catch-specific-type-matches}:
\[
\infer{
  \begin{array}{l}
    \statebrack{
      \textsf{St}(\texttt{Catch}\;
                    (\texttt{throw}(e))\;
                    ((\lfloor(\mathit{pnm?},\tau)\rfloor, \mathit{hnd})::
                     \mathit{hnds}),c),
      \sigma
    }
    \to\\[.5ex]
    \quad \left\langle
      \begin{array}{l}
        \textsf{St}(
           \mathbf{\{}\,[\textsf{VDecInit}\;\tau\;\mathit{pnm}\;
                                            (\textsf{CopyInit}\;e)]\;
                        [\mathit{hnd};\,\textsf{ClearExn}]\,\mathbf{\}}, c),\\
        \sigma[\textsf{current\_exns}\,:=\,e::\textsf{current\_exns}\,]
      \end{array}\right\rangle
  \end{array}
}{
  \begin{array}{c}
  \mathit{pnm} = (\textsf{case}\;\mathit{pnm?}\;\textsf{of}\;*\to \texttt{" no name "}
  \;\;|\;\; \lfloor x \rfloor \to x)\\[.5ex]
  \textsf{exn\_param\_matches}\;\sigma\;\tau\;(\textsf{typeof}(e))
  \end{array}
}
\]
The string \texttt{" no name "} is chosen arbitrarily as the name of
the invisible temporary if the handler has just a type as its
parameter and no associated name.  This name is chosen so as to not
mask any existing names in scope (no legal \cpp{} program can have
variable names that include spaces).

If the declared type $\tau$ matches the type of the exception value,
then the exception value copy-initialises the parameter, and the
handler body is executed.  The constant \textsf{exn\_param\_matches}
checks the match, embodying the rules in \S15.3, paragraph 3.  If
there is no match, then the remaining handlers have to be tried
\ruleid{catch-specific-type-nomatch}:
\[
\infer{
  \begin{array}{c}
    \statebrack{
      \textsf{St}(\texttt{Catch}\;
                    (\texttt{throw}(e))\;
                    ((\lfloor(\mathit{pnm?},\tau)\rfloor, \mathit{hnd})::
                     \mathit{hnds}), c),
      \sigma
    }\\
    \to\\
    \quad\statebrack{
      \textsf{St}(\texttt{Catch}\;(\texttt{throw}(e))\;\mathit{hnds},c),
      \sigma
    }
  \end{array}
}{
  \lnot\,\textsf{exn\_param\_matches}\;\sigma\;\tau\;(\textsf{typeof}(e))
}
\]
If no handlers, remain, the exception propagates further
\ruleid{catch-stmt-empty-hnds} (generalised to allow any statements to
pass through):
\[
\infer{
  \statebrack{
    \textsf{St}(\texttt{Catch}\;s\;[\,], c),
    \sigma
  }
  \to
  \statebrack{\textsf{St}(s, c), \sigma}
}{\rule{0mm}{2ex}}
\]


\subsubsection{Using \texttt{throw} with No Argument}
\label{sec:throw-none}

If flow of control is within an exception handler, or within a
function body that has been called from such, then it is permissible
to use the expression \texttt{throw} without any arguments to cause
the current exception to be rethrown.  This requires the model to
track what the current handled exception is.  More, the
standard requires the state to track the notion of ``most recently
caught'' exception~(\S15.1, paragraph 7), which requires the state to
track a stack of exceptions that have been caught.

The expression version \ethrow{} is converted to the statement form as
soon as it is encountered \ruleid{expression-throw-none}:
\[
\infer{\statebrack{\textsf{Ex}(\ethrow(*),\mathit{se}), \sigma}
  \to
  \statebrack{\textsf{St}(\texttt{throw}(*),c), \sigma}
}{}
\] (The choice of $c$ is again irrelevant.)

There are then two rules for the statement $\texttt{throw}(*)$.  If
there is a current exception, all is well
\ruleid{bare-throw-succeeds}:
\[
\infer{
  \statebrack{\textsf{St}(\texttt{throw}(*), c), \sigma}
  \to
  \statebrack{\textsf{St}(\texttt{throw}(e), c),
    \sigma[\textsf{current\_exns} := \mathit{es}]}
}{
  \sigma.\textsf{current\_exns} = e :: \mathit{es}
}
\]
Otherwise, the program must call the \texttt{std::terminate} function
\[
\infer{
  \statebrack{\textsf{St}(\texttt{throw}(*), c), \sigma}
  \to
  \statebrack{\textsf{St}(\texttt{std::terminate}(), c),\sigma}
}{
  \sigma.\textsf{current\_exn} = [\,]
}
\]

Above, the rules for handlers also use a statement-form
\textsf{ClearExn}.  This special value has the following rule
\ruleid{clear-exn}:
\[
\infer{
\statebrack{\textsf{St}(\textsf{ClearExn},c), \sigma}
\to
\statebrack{\textsf{St}(\textsf{EmptyStmt},c),
  \sigma[\textsf{current\_exns}:=\mathit{es}]}
}{
  \sigma.\textsf{current\_exns} = e::es
}
\]
This ensures that when a handler finishes the most recently caught
exception is no longer recorded as such.  If a handler rethrows the
current exception, or throws a fresh exception of its own, and this
exception escapes the handler, then the flow of control will never
reach the \textsf{ClearExn}, and this rule will not fire.

\subsubsection{Exception Specifications}

The standard's \S15.4 specifies a method whereby functions can specify
which exception types they will produce.  If an unexpected exception
value occurs, this results in a call to \texttt{std::unexpected}.
This is not modelled in the dynamic semantics as it can be emulated
with a compile-time rewriting of the program.  If a function
\texttt{f} is specified to only raise exceptions X and Y, then it can
be rewritten to be
\begin{verbatim}
   f(args)
   {
     try {
       body
     }
     catch (X) { throw; }
     catch (Y) { throw; }
     catch (...) { std::unexpected(); }
   }
\end{verbatim}

\subsubsection{Exceptions and Object Lifetimes}

Exceptions complicate the story about the construction and destruction
of objects.  When a constructor runs it will typically cause a
sequence of sub-objects to also be constructed. If at any stage, an
exception is raised during this process, then those objects that have
been constructed need to have their destructors called, but naturally,
those that haven't yet been constructed shouldn't have destructors
called.

Consider for example
\begin{alltt}
  Class::Class(objty p1) : b1(3), b2(\(e\)) \{ \(\mathit{body}\) \}
\end{alltt}
There are three objects that have their constructors called as a
result of a call to this constructor.  One is the parameter
\texttt{p1}, and the other are the base classes (or data-members)
\texttt{b1} and \texttt{b2}.  When this constructor is called,
\texttt{p1} is always eventually destroyed, but (sub-)objects
\texttt{b1} and \texttt{b2} should live on, unless $\mathit{body}$ or
$e$ cause an exception to be raised.

To get this situation to work in the model, the state's
\texttt{blockclasses} field has to record two sorts of object
construction (which are to be unwound later), sub-object creation and
normal object creation.  When a block exits normally, sub-object
destructors are not called, but instead delayed so that they can be
recorded as having the same lifetime as the enclosing object.  In
Deliverable~3's version of the semantics, all sub-objects were
constructed at the same, uppermost, level, but the addition of
exceptions requires the model to be able to destroy sub-objects
earlier.


\section{Validation}
\label{sec:validation}

Deliverable-2 includes a directory \texttt{holsrcs/testfiles}.  In
this directory there is some preliminary work towards the creation of
a symbolic evaluator, for demonstrating that programs in the model can
behave as one might expect.  This work builds on the ideas
in~\cite{netsem:popl2006}, allowing symbolic exploration of a semantic
definition that features non-deterministic branching.  For the moment,
the code only handles a sequence of external declarations not
requiring any expression evaluation.

In order to make using this tool even easier, I will need to develop
some sort of parser for C++ source code as having to write out
abstract syntax trees by hand is a major annoyance.  It's possible
that the front-end of \texttt{g++} might be usable in this regard.


\section{Omissions}
\label{sec:omissions}

The most significant omission in this semantics is a treatment of
overloading.  This is a complicated feature of the language, but one
that is purely syntactic, and one that is checked and resolved
entirely by the compiler.  If this semantics were to handle
overloading, it would be done in Phases~1~(Name Resolution) and
Phase~2~(Templates).  There a call to a bare \texttt{f} would
ultimately turn into a call into the \texttt{f} whose parameters'
types best matched the types of the actual arguments.  This resolved
call would then be to an exact name and type combination, so that
\texttt{f} might become \texttt{::ns::f(int,char)}.

Similarly, there is no treatment of operator overloading.  Again, any
modelling of this feature would naturally occur in Phases~1 and~2,
where calls to operators such as \texttt{+} would be resolved into
particular calls to functions in particular namespaces and classes.

Two other large omissions in the realm of statics are \texttt{const},
and protection statuses.  In general, the \texttt{const}-ness of an
expression influences the selection of particular functions to call
(more name resolution), and can prevent certain expressions from being
written at all.  These latter constraints are an important part of the
practice of programming with \cpp{}, but again, are checked by the
compiler.  The semantics does model the fact that it is undefined
behaviour to update memory that has been declared as \texttt{const}.


Protection statuses (\ie, the designation of certain fields or base
classes as being \texttt{public}, \texttt{protected} or
\texttt{private}), are similarly a static mechanism, and have almost
no impact on the dynamics of a program.  (They make a difference to
the behaviour of \texttt{dynamic_cast}, and to the dynamic
type-matching that is done in exception handlers.)

I feel that all of these omissions are justified given the commission
to prefer treatment of dynamics rather than static issues.

I believe the most significant dynamic omission is the failure to
support class r-values.

Other language features completely ignored are: enumerated types,
unions, and the \texttt{goto} and \texttt{switch} statements.  Also,
while I provide a rule for post-increment \texttt{++}, there is no
rule for \texttt{--}.

\section{Future}

The final deliverable calls for sanity theorems, namespaces, run-time
type identification and library functions.  All of these save the last
are strightforward.  It is now not clear to me what is meant by
``library functions''; whether actual functions are to be specified,
or whether this is a reference to the mechanisms for linking to
library functions (paragraph three of \S3 of the Statement of Work).

\begin{center}
\rule{0.5\textwidth}{.1mm}
\end{center}

\appendix
\section{Sources}
\label{sec:sources}

The deliverable consists of a compressed \texttt{tar}-file, that when
unpacked consists of a directory called \texttt{qinetiq-cpp}, which in
turn contains four directories
\begin{itemize}
\item \texttt{holsrcs}, containing the HOL source files of the
  mechanisation.  These files will build with the version of HOL4
  present in the CVS repository at SourceForge, with timestamp
  \texttt{2007-05-30 00:00Z}.  See Section~\ref{sec:getting-hol}
  below for instructions on how this version of HOL can be retrieved,
  and how the deliverable's HOL source files can then be built and
  checked.
\item \texttt{talks}, containing the \LaTeX{} source and a PDF for the
  talk presented at the DARP meeting in Newcastle in April 2006.  The
  source assumes that the \texttt{latex-beamer} and \texttt{PSTricks}
  packages are available.
\item \texttt{docs}, containing \LaTeX{} sources and a PDF version of
  this document, as well as sources for the notes on the earlier
  deliverables (nos.~1--4).
\item \texttt{notes}, some \cpp{} source files that illustrate various
  aspects of \cpp{} behaviour.  An accompanying text file explains some
  of the behaviours.
\end{itemize}

\subsection{Building HOL Source-Files}
\label{sec:getting-hol}

\paragraph{Getting HOL From SourceForge}

To get a particular, dated, version of the HOL4 sources from the CVS
repository, one must first issue the command

{\small
\begin{verbatim}
   cvs -d:pserver:anonymous@hol.cvs.sourceforge.net:/cvsroot/hol login
\end{verbatim}
}

When prompted for a password, just press \texttt{Enter} to send a null
response.  The check-out of source code from SourceForge can now
proceed.  The source code fits into 60MB.  Issue the command

{\small
\begin{alltt}
   cvs \textit{repository-spec} co -D \textit{date} hol98
\end{alltt}
}

\noindent where \textit{\ttfamily repository-spec} is the string

{\small
\begin{alltt}
   -d:pserver:anonymous@hol.cvs.sourceforge.net:/cvsroot/hol
\end{alltt}
}

\noindent (also used in the \texttt{login} command), and where
\textit{\ttfamily date} is the desired date, best specified as an
ISO~8601 string enclosed in double-quotes.  For example,
\texttt{"2006-06-30 04:05Z"}.

Once a copy of the sources have been downloaded, further commands can
be used to update this copy to correspond to different dates.  As long
as such commands are issued from within the \texttt{hol98} directory,
the repository specification can be omitted.  The update command is

{\small
\begin{alltt}
   cvs update -d -D \textit{date}
\end{alltt}
}

\paragraph{Installing HOL} Once the sources have been downloaded, the
installation instructions from the page at
\url{http://hol.sourceforge.net} should be followed to build a copy of
HOL.  An installation of the Moscow~ML compiler (v2.01) will also be
required.

\paragraph{Building Deliverable Sources}
When HOL4 has been installed, the \texttt{Holmake} program (found in
the \texttt{hol98/bin} directory) can be run in the \texttt{holsrcs}
directory to create and check the logical theories.


\bibliographystyle{plain}
\bibliography{deliverables}

\printindex

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
